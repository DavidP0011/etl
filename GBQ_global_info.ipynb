{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlXOuJB+Wil4z0rWpFU5HK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidP0011/etl/blob/main/GBQ_global_info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INICIALIZACIÓN"
      ],
      "metadata": {
        "id": "k194DOe_R2LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title INSTALACIÓN DE LIBRERÍAS\n"
      ],
      "metadata": {
        "id": "1Zns1Qz-R4ca"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZQllQ2vuRXAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78309653-3cd2-4dfc-a909-c7c8ec4d9988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[google.colab authenticate_user]:\n",
            "Authenticated!\n",
            "\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title IMPORTACIÓN DE LIBRERÍAS SISTEMA\n",
        "\n",
        "import pandas as pd\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('[google.colab authenticate_user]:\\nAuthenticated!\\n')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GBQ_tables_fields_info_df\n",
        "def GBQ_tables_fields_info_df(config):\n",
        "    \"\"\"\n",
        "    Retorna un DataFrame con la información de datasets, tablas y campos de un proyecto de BigQuery.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Diccionario de configuración con:\n",
        "            - 'project_id' (str) [requerido]: El ID del proyecto de BigQuery.\n",
        "            - 'datasets' (list) [opcional]: Lista de los IDs de los datasets a consultar.\n",
        "              Si no se proporciona, se consultan todos los disponibles en el proyecto.\n",
        "            - 'include_tables' (bool) [opcional]: Indica si se deben incluir las tablas\n",
        "              en el esquema. Por defecto es True.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame con las columnas:\n",
        "            [\n",
        "                'project_id',\n",
        "                'dataset_id',\n",
        "                'table_name',\n",
        "                'field_name',\n",
        "                'field_type',\n",
        "                'num_rows',\n",
        "                'num_columns',\n",
        "                'size_mb'\n",
        "            ]\n",
        "    \"\"\"\n",
        "    from google.cloud import bigquery\n",
        "    from google.colab import auth\n",
        "    import pandas as pd\n",
        "\n",
        "    # 1. Autenticamos (en caso de estar en Colab)\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    project_id = config.get('project_id')\n",
        "    if not project_id:\n",
        "        raise ValueError(\"El 'project_id' es un argumento requerido en el diccionario de configuración.\")\n",
        "\n",
        "    # Parámetros adicionales\n",
        "    datasets_incluidos = config.get('datasets', None)\n",
        "    include_tables = config.get('include_tables', True)\n",
        "\n",
        "    # 2. Creamos el cliente de BigQuery\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # 3. Obtenemos la lista de datasets\n",
        "    if datasets_incluidos:\n",
        "        datasets = [client.get_dataset(f\"{project_id}.{dataset_id}\") for dataset_id in datasets_incluidos]\n",
        "    else:\n",
        "        datasets = list(client.list_datasets(project=project_id))\n",
        "\n",
        "    # Estructura temporal para ir almacenando la info de tablas y campos\n",
        "    tables_info_list = []\n",
        "\n",
        "    for dataset in datasets:\n",
        "        dataset_id = dataset.dataset_id\n",
        "        full_dataset_id = f\"{project_id}.{dataset_id}\"\n",
        "\n",
        "        # 4. Si include_tables=True, listamos las tablas de cada dataset\n",
        "        if include_tables:\n",
        "            tables = list(client.list_tables(full_dataset_id))\n",
        "            for table_item in tables:\n",
        "                table_ref = client.get_table(table_item.reference)  # Obtenemos objeto Table\n",
        "\n",
        "                # Datos de la tabla\n",
        "                table_name = table_item.table_id\n",
        "                num_rows = table_ref.num_rows\n",
        "                num_columns = len(table_ref.schema)\n",
        "                size_mb = table_ref.num_bytes / (1024 * 1024)  # Convertir de bytes a MB\n",
        "\n",
        "                # 5. Campos (schema) de la tabla\n",
        "                fields = table_ref.schema\n",
        "\n",
        "                # Cada campo de la tabla se agrega como un registro independiente\n",
        "                if fields:\n",
        "                    for field in fields:\n",
        "                        tables_info_list.append({\n",
        "                            'project_id': project_id,\n",
        "                            'dataset_id': dataset_id,\n",
        "                            'table_name': table_name,\n",
        "                            'field_name': field.name,\n",
        "                            'field_type': field.field_type,\n",
        "                            'num_rows': num_rows,\n",
        "                            'num_columns': num_columns,\n",
        "                            'size_mb': round(size_mb, 2)\n",
        "                        })\n",
        "                else:\n",
        "                    # Si la tabla no tiene campos, se ingresa un registro con field_name=None\n",
        "                    tables_info_list.append({\n",
        "                        'project_id': project_id,\n",
        "                        'dataset_id': dataset_id,\n",
        "                        'table_name': table_name,\n",
        "                        'field_name': None,\n",
        "                        'field_type': None,\n",
        "                        'num_rows': num_rows,\n",
        "                        'num_columns': num_columns,\n",
        "                        'size_mb': round(size_mb, 2)\n",
        "                    })\n",
        "        else:\n",
        "            # En caso de no incluir tablas, añadimos el dataset sin info de tablas\n",
        "            # (aunque rara vez se desea esto en un DataFrame).\n",
        "            tables_info_list.append({\n",
        "                'project_id': project_id,\n",
        "                'dataset_id': dataset_id,\n",
        "                'table_name': None,\n",
        "                'field_name': None,\n",
        "                'field_type': None,\n",
        "                'num_rows': None,\n",
        "                'num_columns': None,\n",
        "                'size_mb': None\n",
        "            })\n",
        "\n",
        "    # 6. Convertimos la lista de diccionarios en DataFrame\n",
        "    df_tables_fields = pd.DataFrame(tables_info_list)\n",
        "\n",
        "    return df_tables_fields"
      ],
      "metadata": {
        "id": "oetuSTbNZzTx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title df_to_GSheets()\n",
        "\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import pandas as pd\n",
        "\n",
        "def df_to_GSheets(dic):\n",
        "    \"\"\"\n",
        "    Volca el contenido de un DataFrame en una hoja de Google Sheets.\n",
        "\n",
        "    Parámetros:\n",
        "        dic (dict): Diccionario con las siguientes claves:\n",
        "            - 'df': DataFrame a volcar.\n",
        "            - 'spreadsheet_id': URL completa o ID del spreadsheet de Google Sheets.\n",
        "            - 'worksheet_name': Nombre de la pestaña (worksheet) destino.\n",
        "            - 'json_keyfile': Ruta al archivo JSON con las credenciales.\n",
        "\n",
        "    La función se conecta a Google Sheets, busca (o crea) la pestaña indicada,\n",
        "    limpia su contenido y volca el DataFrame.\n",
        "    \"\"\"\n",
        "    # Autenticación mediante el archivo de credenciales JSON\n",
        "    gc = gspread.service_account(filename=dic['json_keyfile'])\n",
        "\n",
        "    # Comprobar si se ha pasado la URL completa o solo el ID y abrir el spreadsheet correspondiente\n",
        "    spreadsheet_ref = dic['spreadsheet_id']\n",
        "    if spreadsheet_ref.startswith(\"http\"):\n",
        "        sh = gc.open_by_url(spreadsheet_ref)\n",
        "    else:\n",
        "        sh = gc.open_by_key(spreadsheet_ref)\n",
        "\n",
        "    # Intentar obtener la hoja especificada; si no existe, se crea\n",
        "    try:\n",
        "        worksheet = sh.worksheet(dic['worksheet_name'])\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        # Crear una nueva hoja con tamaño arbitrario (ajustar según necesidad)\n",
        "        worksheet = sh.add_worksheet(title=dic['worksheet_name'], rows=\"100\", cols=\"20\")\n",
        "\n",
        "    # Limpiar el contenido previo de la hoja\n",
        "    worksheet.clear()\n",
        "\n",
        "    # Volcar el DataFrame en la hoja de cálculo\n",
        "    set_with_dataframe(worksheet, dic['df'])\n",
        "\n",
        "    print(\"DataFrame volcado correctamente en la hoja '{}' del spreadsheet.\".format(\n",
        "        dic['worksheet_name']\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "vRpmydM5Ug9R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJECUCIONES"
      ],
      "metadata": {
        "id": "ZE0MFB5WZBUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GBQ INFO GLOBAL\n",
        "%%time\n",
        "\n",
        "# Configuración\n",
        "project_id = 'animum-dev-datawarehouse' # @param {\"type\":\"string\"}\n",
        "datasets = [ ]                          # @param {\"type\":\"string\"}\n",
        "include_tables = True                   # @param {\"type\":\"boolean\"}\n",
        "\n",
        "config = {\n",
        "    'project_id': project_id,\n",
        "    'datasets': datasets,\n",
        "    'include_tables': include_tables\n",
        "}\n",
        "\n",
        "# Llamada a la nueva función\n",
        "full_info_from_GBQ_df = GBQ_tables_fields_info_df(config)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Visualizar\n",
        "\n",
        "# display(full_info_from_GBQ_df)\n",
        "\n",
        "\n",
        "# Restaurar configuración de pandas\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n"
      ],
      "metadata": {
        "id": "bRQQ51emRZBK",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f913c3cc-1622-4f77-ce1f-21243e778103"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.12 s, sys: 210 ms, total: 2.33 s\n",
            "Wall time: 1min 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GBQ DATASETS\n",
        "print(\"Datasets disponibles:\\n\")\n",
        "for dataset in full_info_from_GBQ_df['dataset_id'].unique():\n",
        "    print(f\"{dataset}\")\n"
      ],
      "metadata": {
        "id": "2_838s1URtaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34710576-6f86-4dea-9939-7c235389baa1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets disponibles:\n",
            "\n",
            "BOMojo_staging_01\n",
            "FAds_staging_v01\n",
            "GAds_staging_v01\n",
            "IMDb_raw_01\n",
            "IMDb_staging_01\n",
            "cd2_01raw_01\n",
            "facebook_ads_raw_v01\n",
            "facebook_ads_raw_v01_facebook_ads\n",
            "facebook_ads_raw_v01_facebook_ads_source\n",
            "fivetran_metadata\n",
            "fivetran_metadata_fivetran_platform\n",
            "fivetran_metadata_stg_fivetran_platform\n",
            "google_ads_raw_01\n",
            "google_ads_raw_01_google_ads\n",
            "google_ads_raw_01_google_ads_source\n",
            "hubspot_BI_v01\n",
            "hubspot_raw_v01\n",
            "hubspot_raw_v01_hubspot\n",
            "hubspot_raw_v01_stg_hubspot\n",
            "hubspot_staging_v01\n",
            "mkt_02st_01\n",
            "tablas_mapeo\n",
            "tp_02st_01\n",
            "tp_03bi_01\n",
            "vl_01raw_01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SUBIDA A GOOGLE SHEETS\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Diccionario con los parámetros (usando URL completa)\n",
        "parametros = {\n",
        "    'df': full_info_from_GBQ_df,  # o full_info_from_GBQ_df si ese es tu DataFrame real\n",
        "    'spreadsheet_id': 'https://docs.google.com/spreadsheets/d/1aJCGTJtDu_ODqBc4zUcrpQ-q6PE_HN0rO4mwYMIhCXw',\n",
        "    'worksheet_name': 'DATA',\n",
        "    'json_keyfile': '/content/drive/Othercomputers/Mi PC/proyectos/api_keys/animum-dev-datawarehouse-ef58845eb41a.json'\n",
        "}\n",
        "\n",
        "# Volcamos el DataFrame en la hoja de Google Sheets\n",
        "df_to_GSheets(parametros)\n"
      ],
      "metadata": {
        "id": "PoNBjIJNRwxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdfede01-b996-4b3e-e0a1-7e42b2eeac34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame volcado correctamente en la hoja 'DATA' del spreadsheet.\n"
          ]
        }
      ]
    }
  ]
}