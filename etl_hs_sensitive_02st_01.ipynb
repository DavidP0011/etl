{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNgQSwTNB8FtPHir9g9X9ic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ***NOTEBOOK EN GBQ: \"Cargas de trabajo - etl_tp\"***"],"metadata":{"id":"b850MT3xK5QB"}},{"cell_type":"markdown","source":["# INICIALIZACIÓN"],"metadata":{"id":"SV1jWDmAcEBN"}},{"cell_type":"code","source":["# @title INSTALACIÓN DE LIBRERÍAS\n","\n","# !pip install google-cloud-secret-manager\n"],"metadata":{"id":"IOXVVtB8cJFu","executionInfo":{"status":"ok","timestamp":1738654267060,"user_tz":-60,"elapsed":242,"user":{"displayName":"David Plaza","userId":"17308742384241259995"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# @title IMPORTACIÓN DE LIBRERÍAS GCP\n","\n","from google.cloud import storage\n","import os\n","\n","from google.cloud import bigquery  # Importa el cliente de BigQuery\n","from google.cloud import storage   # Importa el cliente de Cloud Storage\n","import pandas as pd\n","import io\n","import re\n","import unicodedata\n","import chardet  # Biblioteca para detectar codificaciones de texto\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.cloud import secretmanager\n","import os\n","\n","# Configura el cliente de Secret Manager\n","client = secretmanager.SecretManagerServiceClient()\n","project_id = \"animum-dev-datawarehouse\"\n","\n","# Función para obtener un secreto por su nombre\n","def get_secret(secret_id):\n","    name = f\"projects/{project_id}/secrets/{secret_id}/versions/latest\"\n","    response = client.access_secret_version(request={\"name\": name})\n","    return response.payload.data.decode(\"UTF-8\")\n","\n","# Obtén los valores de los secretos\n","hs_datawarehouse_acces_token = get_secret(\"hs_datawarehouse_acces_token\")\n","hs_datawarehouse_secret_key = get_secret(\"hs_datawarehouse_secret_key\")\n","\n","# Configura las variables de entorno\n","os.environ['hs_datawarehouse_acces_token'] = hs_datawarehouse_acces_token\n","os.environ['hs_datawarehouse_secret_key'] = hs_datawarehouse_secret_key\n","\n","print(\"Secretos configurados correctamente.\")"],"metadata":{"id":"CPamw_5plJ6H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738654268525,"user_tz":-60,"elapsed":1222,"user":{"displayName":"David Plaza","userId":"17308742384241259995"}},"outputId":"aff87624-1790-4b8b-f6f4-7d5fefde0ebf"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Secretos configurados correctamente.\n"]}]},{"cell_type":"code","source":["# @title HS_to_GBQ_sensitive_data()\n","def HS_to_GBQ_sensitive_data(params: dict):\n","    \"\"\"\n","    Extrae contactos de HubSpot en chunks de hasta 10k contactos cada uno,\n","    filtrando por un rango de 'createdate', y sube cada chunk a BigQuery.\n","\n","    Args:\n","        params (dict):\n","            - HS_api_key (str): API key de HubSpot o token de Private App.\n","            - GBQ_project_id (str): ID del proyecto de BigQuery.\n","            - GCS_bucket_name (str): Nombre del bucket de GCS.\n","            - GBQ_dataset_id (str): Dataset de BigQuery.\n","            - GBQ_table_id (str): Tabla de destino en BigQuery.\n","            - HS_fields_no_sensitive_names_list (list): Lista de campos NO sensibles a recuperar\n","              de HubSpot (p.ej., [\"email\"]). Se utilizarán para paginar; se forzarán \"id\" y \"createdate\".\n","            - HS_fields_sensitive_names_list (list): Lista de campos SENSIBLES a recuperar vía batch/read\n","              (p.ej., [\"iban\", \"codigo_bic_swift\", \"documento_nacional_de_identidad_numero\"]).\n","            - HS_api_lines_per_call (int): Límite de registros por llamada a la API de HubSpot.\n","            - hs_contact_filter_createdate (dict): Filtro de fechas, por ejemplo:\n","              {\"from\": \"2024-01-01\", \"to\": \"2025-02-02\", \"mode\": \"between\"}.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    import requests\n","    import pandas as pd\n","    from google.colab import auth\n","    from google.cloud import bigquery, storage\n","    from datetime import datetime, timedelta\n","    import os\n","    import uuid\n","\n","    # ========== LECTURA DE PARÁMETROS ==========\n","    HS_api_key = params[\"HS_api_key\"]\n","    GBQ_project_id = params[\"GBQ_project_id\"]\n","    GCS_bucket_name = params[\"GCS_bucket_name\"]\n","    GBQ_dataset_id = params[\"GBQ_dataset_id\"]\n","    GBQ_table_id = params[\"GBQ_table_id\"]\n","    HS_api_lines_per_call = params[\"HS_api_lines_per_call\"]\n","    createdate_filter = params[\"hs_contact_filter_createdate\"]\n","\n","    hs_fields_no_sensitive = params[\"HS_fields_no_sensitive_names_list\"]\n","    hs_fields_sensitive = params[\"HS_fields_sensitive_names_list\"]\n","\n","    # Forzamos que \"id\" y \"createdate\" estén en la parte no sensible (son obligatorios)\n","    mandatory = {\"id\", \"createdate\"}\n","    for field in mandatory:\n","        if field not in hs_fields_no_sensitive:\n","            hs_fields_no_sensitive.append(field)\n","\n","    # Convertir las fechas a datetime; 'to' se ajusta a las 23:59:59\n","    from_date = datetime.strptime(createdate_filter[\"from\"], \"%Y-%m-%d\")\n","    to_date = datetime.strptime(createdate_filter[\"to\"], \"%Y-%m-%d\") + timedelta(days=1, seconds=-1)\n","\n","    non_sens = sorted(hs_fields_no_sensitive)\n","    sens = sorted(hs_fields_sensitive)\n","\n","    print(\"=== Comenzando extracción de contactos HubSpot ===\")\n","    print(f\"Parámetros de ejecución:\\n\"\n","          f\"  - Rango de fechas: {from_date} a {to_date}\\n\"\n","          f\"  - Campos no sensibles (para búsqueda): {non_sens}\\n\"\n","          f\"  - Campos sensibles (batch read): {sens}\\n\")\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    # FUNCIÓN: BUSCAR CONTACTOS (SIN CAMPOS SENSIBLES) POR RANGO DE FECHAS\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    def hubspot_search_chunk(start_date, end_date, limit_chunk=10000):\n","        print(f\"   [INFO] Extrayendo contactos desde {start_date.isoformat()} hasta {end_date.isoformat()}. Límite total: {limit_chunk}\")\n","        start_iso = start_date.isoformat(timespec=\"seconds\") + \"Z\"\n","        end_iso = end_date.isoformat(timespec=\"seconds\") + \"Z\"\n","        url_search = \"https://api.hubapi.com/crm/v3/objects/contacts/search\"\n","        headers = {\"Authorization\": f\"Bearer {HS_api_key}\", \"Content-Type\": \"application/json\"}\n","        accumulated = []\n","        after = None\n","        while len(accumulated) < limit_chunk:\n","            remaining = limit_chunk - len(accumulated)\n","            body = {\n","                \"filterGroups\": [\n","                    {\n","                        \"filters\": [\n","                            {\"propertyName\": \"createdate\", \"operator\": \"GTE\", \"value\": start_iso},\n","                            {\"propertyName\": \"createdate\", \"operator\": \"LTE\", \"value\": end_iso},\n","                        ]\n","                    }\n","                ],\n","                \"sorts\": [{\"propertyName\": \"createdate\", \"direction\": \"ASCENDING\"}],\n","                \"properties\": non_sens,\n","                \"limit\": min(HS_api_lines_per_call, remaining),\n","            }\n","            if after:\n","                body[\"after\"] = after\n","            resp = requests.post(url_search, headers=headers, json=body)\n","            if resp.status_code != 200:\n","                raise requests.HTTPError(f\"❌ Error en /search: {resp.text}\")\n","            data = resp.json()\n","            results = data.get(\"results\", [])\n","            if not results:\n","                break\n","            for r in results:\n","                props = r.get(\"properties\", {})\n","                row = {}\n","                for c in non_sens:\n","                    if c == \"id\":\n","                        row[c] = r.get(\"id\")  # Extraer 'id' del nivel superior\n","                    else:\n","                        row[c] = props.get(c)\n","                accumulated.append(row)\n","            after = data.get(\"paging\", {}).get(\"next\", {}).get(\"after\")\n","            if not after:\n","                break\n","        if not accumulated:\n","            return [], None, False\n","        last_cdate_str = accumulated[-1].get(\"createdate\")\n","        if last_cdate_str is not None:\n","            try:\n","                last_cdate_dt = datetime.fromisoformat(last_cdate_str.replace(\"Z\", \"\"))\n","            except ValueError:\n","                last_cdate_dt = None\n","        else:\n","            last_cdate_dt = None\n","        reached = (len(accumulated) >= limit_chunk)\n","        return accumulated, last_cdate_dt, reached\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    # FUNCIÓN: BATCH READ DE CAMPOS SENSIBLES DADO UNA LISTA DE CONTACTOS\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    def hubspot_batch_sens(contact_rows):\n","        import math\n","        if not sens:\n","            print(\"   [INFO] No se han solicitado campos sensibles. Saltamos batch read.\")\n","            return pd.DataFrame(contact_rows)\n","        if not contact_rows:\n","            print(\"   [INFO] Lista vacía de contactos. Saltamos batch read.\")\n","            return pd.DataFrame([])\n","        ids = [x.get(\"id\") for x in contact_rows if x.get(\"id\")]\n","        if not ids:\n","            print(\"   [WARNING] Ningún contacto con 'id'. Saltamos batch read.\")\n","            return pd.DataFrame(contact_rows)\n","        print(f\"   [INFO] Recuperando {len(sens)} propiedades sensibles para {len(ids)} contactos vía batch/read.\")\n","        chunk_size = 100\n","        total_ids = len(ids)\n","        n_chunks = math.ceil(total_ids / chunk_size)\n","        url_batch = \"https://api.hubapi.com/crm/v3/objects/contacts/batch/read\"\n","        headers = {\"Authorization\": f\"Bearer {HS_api_key}\", \"Content-Type\": \"application/json\"}\n","        id_to_sens = {}\n","        for i in range(n_chunks):\n","            subset_ids = ids[i*chunk_size : (i+1)*chunk_size]\n","            body_b = {\n","                \"properties\": sens,\n","                \"inputs\": [{\"id\": cid} for cid in subset_ids]\n","            }\n","            resp_b = requests.post(url_batch, headers=headers, json=body_b)\n","            if resp_b.status_code != 200:\n","                raise requests.HTTPError(f\"❌ Error en batch/read: {resp_b.text}\")\n","            data_b = resp_b.json()\n","            results_b = data_b.get(\"results\", [])\n","            for rb in results_b:\n","                c_id = rb.get(\"id\")\n","                p_b = rb.get(\"properties\", {})\n","                id_to_sens[c_id] = {s: p_b.get(s) for s in sens}\n","            print(f\"      - Chunk {i+1}/{n_chunks} => {len(results_b)} contactos OK.\")\n","        df_no_sens = pd.DataFrame(contact_rows)\n","        for sfield in sens:\n","            df_no_sens[sfield] = df_no_sens[\"id\"].apply(lambda c_id: id_to_sens.get(c_id, {}).get(sfield))\n","        return df_no_sens\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    # FUNCIÓN: SUBIR A BIGQUERY (USANDO CSV TEMPORAL EN GCS)\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    def upload_df_to_bq(df, disposition=\"WRITE_APPEND\"):\n","        if df.empty:\n","            print(\"   [WARNING] DataFrame vacío, no se subirá nada.\")\n","            return\n","        auth.authenticate_user()\n","        print(f\"   [INFO] Subiendo DataFrame con {len(df)} filas a BigQuery (disposition={disposition}).\")\n","        temp_file_local = \"temp_contacts_chunk.csv\"\n","        df.to_csv(temp_file_local, index=False)\n","        storage_client = storage.Client(project=GBQ_project_id)\n","        bucket = storage_client.bucket(GCS_bucket_name)\n","        blob_name = f\"temp_contacts_{uuid.uuid4().hex}.csv\"\n","        blob = bucket.blob(blob_name)\n","        print(f\"      - Subiendo CSV a gs://{GCS_bucket_name}/{blob_name} ...\")\n","        blob.upload_from_filename(temp_file_local)\n","        gcs_uri = f\"gs://{GCS_bucket_name}/{blob_name}\"\n","        bq_client = bigquery.Client(project=GBQ_project_id)\n","        table_id = f\"{GBQ_project_id}.{GBQ_dataset_id}.{GBQ_table_id}\"\n","        job_config = bigquery.LoadJobConfig(\n","            write_disposition=disposition,\n","            source_format=bigquery.SourceFormat.CSV,\n","            autodetect=True,\n","        )\n","        try:\n","            print(f\"      - Cargando en BQ => {table_id} ...\")\n","            load_job = bq_client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)\n","            load_job.result()\n","            print(f\"      ✅ Carga completada. Se subieron {len(df)} filas.\")\n","        finally:\n","            blob.delete()\n","            if os.path.exists(temp_file_local):\n","                os.remove(temp_file_local)\n","\n","    # ========== LÓGICA PRINCIPAL: DIVIDIR EN CHUNKS ==========\n","    chunk_index = 0\n","    current_start = from_date\n","    print(\"=== Iniciando loop de extracción por chunks de hasta 10k contactos ===\")\n","    while current_start <= to_date:\n","        chunk_index += 1\n","        print(f\"=== Procesando chunk #{chunk_index} ===\")\n","        contacts, last_cdate, reached_limit = hubspot_search_chunk(current_start, to_date)\n","        if not contacts:\n","            print(\"   [INFO] No se encontraron más contactos en este sub-rango.\")\n","            break\n","        print(f\"   [INFO] {len(contacts)} contactos extraídos en este chunk. Iniciando batch read de campos sensibles...\")\n","        df_chunk = hubspot_batch_sens(contacts)\n","        disposition = \"WRITE_TRUNCATE\" if chunk_index == 1 else \"WRITE_APPEND\"\n","        upload_df_to_bq(df_chunk, disposition=disposition)\n","        if not last_cdate:\n","            print(\"   [WARNING] No se pudo determinar la última fecha 'createdate'. Terminando...\")\n","            break\n","        next_start = last_cdate + timedelta(seconds=1)\n","        current_start = next_start\n","        if not reached_limit:\n","            print(\"   [INFO] No se alcanzó el límite de 10k. No hay más contactos en el rango.\")\n","            break\n","    print(\"=== Extracción y carga completadas exitosamente. ===\")\n"],"metadata":{"id":"92qRakm85joV","executionInfo":{"status":"ok","timestamp":1738658714499,"user_tz":-60,"elapsed":1222,"user":{"displayName":"David Plaza","userId":"17308742384241259995"}}},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":["# EJECUCIONES"],"metadata":{"id":"SDY2ZxTa_G-x"}},{"cell_type":"code","source":["# @title IMPORTACIÓN DATOS SENSIBLES HS TO GBQ\n","hs_contact_filter_createdate_from = \"2024-01-01\"  # @param {\"type\":\"date\"}\n","hs_contact_filter_createdate_to   = \"2025-02-02\"  # @param {\"type\":\"date\"}\n","\n","params = {\n","    \"HS_api_key\": hs_datawarehouse_acces_token,  # Tu token o API key de HubSpot\n","    \"GBQ_project_id\": \"animum-dev-datawarehouse\",  # ID de tu proyecto en BigQuery\n","    \"GCS_bucket_name\": \"temp_datawarehouse\",         # Nombre del bucket en GCS para archivos temporales\n","    \"GBQ_dataset_id\": \"tp_02st_01\",                   # Dataset en BigQuery\n","    \"GBQ_table_id\": \"hs_contact_sensitive_cleaned\",   # Tabla destino en BigQuery\n","    \"HS_fields_no_sensitive_names_list\": [           # Lista de campos NO sensibles (obligatoriamente debe incluir \"id\" y \"createdate\")\n","        \"email\"                                      # Ejemplo: se puede incluir \"email\"\n","    ],\n","    \"HS_fields_sensitive_names_list\": [              # Lista de campos SENSIBLES a recuperar vía batch/read\n","        \"iban\",\n","        \"codigo_bic_swift\",\n","        \"documento_nacional_de_identidad_numero\"\n","    ],\n","    \"HS_api_lines_per_call\": 100,                     # Número de registros por llamada a la API de HubSpot\n","    \"hs_contact_filter_createdate\": {                 # Filtro de fechas para la extracción\n","        \"from\": hs_contact_filter_createdate_from,                         # Fecha de inicio (YYYY-MM-DD)\n","        \"to\": hs_contact_filter_createdate_to,                           # Fecha de fin (YYYY-MM-DD)\n","        \"mode\": \"between\"\n","    }\n","}\n","\n","\n","HS_to_GBQ_sensitive_data(params)\n"],"metadata":{"id":"DBiZbrnB_EZr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"036db926-e76d-4a61-876c-695b86ac5a5b","executionInfo":{"status":"ok","timestamp":1738658811832,"user_tz":-60,"elapsed":95404,"user":{"displayName":"David Plaza","userId":"17308742384241259995"}}},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Comenzando extracción de contactos HubSpot ===\n","Parámetros de ejecución:\n","  - Rango de fechas: 2024-01-01 00:00:00 a 2025-02-02 23:59:59\n","  - Campos no sensibles (para búsqueda): ['createdate', 'email', 'id']\n","  - Campos sensibles (batch read): ['codigo_bic_swift', 'documento_nacional_de_identidad_numero', 'iban']\n","\n","=== Iniciando loop de extracción por chunks de hasta 10k contactos ===\n","=== Procesando chunk #1 ===\n","   [INFO] Extrayendo contactos desde 2024-01-01T00:00:00 hasta 2025-02-02T23:59:59. Límite total: 10000\n","   [INFO] 10000 contactos extraídos en este chunk. Iniciando batch read de campos sensibles...\n","   [INFO] Recuperando 3 propiedades sensibles para 10000 contactos vía batch/read.\n","      - Chunk 1/100 => 100 contactos OK.\n","      - Chunk 2/100 => 100 contactos OK.\n","      - Chunk 3/100 => 100 contactos OK.\n","      - Chunk 4/100 => 100 contactos OK.\n","      - Chunk 5/100 => 100 contactos OK.\n","      - Chunk 6/100 => 100 contactos OK.\n","      - Chunk 7/100 => 100 contactos OK.\n","      - Chunk 8/100 => 100 contactos OK.\n","      - Chunk 9/100 => 100 contactos OK.\n","      - Chunk 10/100 => 100 contactos OK.\n","      - Chunk 11/100 => 100 contactos OK.\n","      - Chunk 12/100 => 100 contactos OK.\n","      - Chunk 13/100 => 100 contactos OK.\n","      - Chunk 14/100 => 100 contactos OK.\n","      - Chunk 15/100 => 100 contactos OK.\n","      - Chunk 16/100 => 100 contactos OK.\n","      - Chunk 17/100 => 100 contactos OK.\n","      - Chunk 18/100 => 100 contactos OK.\n","      - Chunk 19/100 => 100 contactos OK.\n","      - Chunk 20/100 => 100 contactos OK.\n","      - Chunk 21/100 => 100 contactos OK.\n","      - Chunk 22/100 => 100 contactos OK.\n","      - Chunk 23/100 => 100 contactos OK.\n","      - Chunk 24/100 => 100 contactos OK.\n","      - Chunk 25/100 => 100 contactos OK.\n","      - Chunk 26/100 => 100 contactos OK.\n","      - Chunk 27/100 => 100 contactos OK.\n","      - Chunk 28/100 => 100 contactos OK.\n","      - Chunk 29/100 => 100 contactos OK.\n","      - Chunk 30/100 => 100 contactos OK.\n","      - Chunk 31/100 => 100 contactos OK.\n","      - Chunk 32/100 => 100 contactos OK.\n","      - Chunk 33/100 => 100 contactos OK.\n","      - Chunk 34/100 => 100 contactos OK.\n","      - Chunk 35/100 => 100 contactos OK.\n","      - Chunk 36/100 => 100 contactos OK.\n","      - Chunk 37/100 => 100 contactos OK.\n","      - Chunk 38/100 => 100 contactos OK.\n","      - Chunk 39/100 => 100 contactos OK.\n","      - Chunk 40/100 => 100 contactos OK.\n","      - Chunk 41/100 => 100 contactos OK.\n","      - Chunk 42/100 => 100 contactos OK.\n","      - Chunk 43/100 => 100 contactos OK.\n","      - Chunk 44/100 => 100 contactos OK.\n","      - Chunk 45/100 => 100 contactos OK.\n","      - Chunk 46/100 => 100 contactos OK.\n","      - Chunk 47/100 => 100 contactos OK.\n","      - Chunk 48/100 => 100 contactos OK.\n","      - Chunk 49/100 => 100 contactos OK.\n","      - Chunk 50/100 => 100 contactos OK.\n","      - Chunk 51/100 => 100 contactos OK.\n","      - Chunk 52/100 => 100 contactos OK.\n","      - Chunk 53/100 => 100 contactos OK.\n","      - Chunk 54/100 => 100 contactos OK.\n","      - Chunk 55/100 => 100 contactos OK.\n","      - Chunk 56/100 => 100 contactos OK.\n","      - Chunk 57/100 => 100 contactos OK.\n","      - Chunk 58/100 => 100 contactos OK.\n","      - Chunk 59/100 => 100 contactos OK.\n","      - Chunk 60/100 => 100 contactos OK.\n","      - Chunk 61/100 => 100 contactos OK.\n","      - Chunk 62/100 => 100 contactos OK.\n","      - Chunk 63/100 => 100 contactos OK.\n","      - Chunk 64/100 => 100 contactos OK.\n","      - Chunk 65/100 => 100 contactos OK.\n","      - Chunk 66/100 => 100 contactos OK.\n","      - Chunk 67/100 => 100 contactos OK.\n","      - Chunk 68/100 => 100 contactos OK.\n","      - Chunk 69/100 => 100 contactos OK.\n","      - Chunk 70/100 => 100 contactos OK.\n","      - Chunk 71/100 => 100 contactos OK.\n","      - Chunk 72/100 => 100 contactos OK.\n","      - Chunk 73/100 => 100 contactos OK.\n","      - Chunk 74/100 => 100 contactos OK.\n","      - Chunk 75/100 => 100 contactos OK.\n","      - Chunk 76/100 => 100 contactos OK.\n","      - Chunk 77/100 => 100 contactos OK.\n","      - Chunk 78/100 => 100 contactos OK.\n","      - Chunk 79/100 => 100 contactos OK.\n","      - Chunk 80/100 => 100 contactos OK.\n","      - Chunk 81/100 => 100 contactos OK.\n","      - Chunk 82/100 => 100 contactos OK.\n","      - Chunk 83/100 => 100 contactos OK.\n","      - Chunk 84/100 => 100 contactos OK.\n","      - Chunk 85/100 => 100 contactos OK.\n","      - Chunk 86/100 => 100 contactos OK.\n","      - Chunk 87/100 => 100 contactos OK.\n","      - Chunk 88/100 => 100 contactos OK.\n","      - Chunk 89/100 => 100 contactos OK.\n","      - Chunk 90/100 => 100 contactos OK.\n","      - Chunk 91/100 => 100 contactos OK.\n","      - Chunk 92/100 => 100 contactos OK.\n","      - Chunk 93/100 => 100 contactos OK.\n","      - Chunk 94/100 => 100 contactos OK.\n","      - Chunk 95/100 => 100 contactos OK.\n","      - Chunk 96/100 => 100 contactos OK.\n","      - Chunk 97/100 => 100 contactos OK.\n","      - Chunk 98/100 => 100 contactos OK.\n","      - Chunk 99/100 => 100 contactos OK.\n","      - Chunk 100/100 => 100 contactos OK.\n","   [INFO] Subiendo DataFrame con 10000 filas a BigQuery (disposition=WRITE_TRUNCATE).\n","      - Subiendo CSV a gs://temp_datawarehouse/temp_contacts_210c65c664614d6c87f681832041c3f0.csv ...\n","      - Cargando en BQ => animum-dev-datawarehouse.tp_02st_01.hs_contact_sensitive_cleaned ...\n","      ✅ Carga completada. Se subieron 10000 filas.\n","=== Procesando chunk #2 ===\n","   [INFO] Extrayendo contactos desde 2024-10-07T05:41:50.974000 hasta 2025-02-02T23:59:59. Límite total: 10000\n","   [INFO] 5777 contactos extraídos en este chunk. Iniciando batch read de campos sensibles...\n","   [INFO] Recuperando 3 propiedades sensibles para 5777 contactos vía batch/read.\n","      - Chunk 1/58 => 100 contactos OK.\n","      - Chunk 2/58 => 100 contactos OK.\n","      - Chunk 3/58 => 100 contactos OK.\n","      - Chunk 4/58 => 100 contactos OK.\n","      - Chunk 5/58 => 100 contactos OK.\n","      - Chunk 6/58 => 100 contactos OK.\n","      - Chunk 7/58 => 100 contactos OK.\n","      - Chunk 8/58 => 100 contactos OK.\n","      - Chunk 9/58 => 100 contactos OK.\n","      - Chunk 10/58 => 100 contactos OK.\n","      - Chunk 11/58 => 100 contactos OK.\n","      - Chunk 12/58 => 100 contactos OK.\n","      - Chunk 13/58 => 100 contactos OK.\n","      - Chunk 14/58 => 100 contactos OK.\n","      - Chunk 15/58 => 100 contactos OK.\n","      - Chunk 16/58 => 100 contactos OK.\n","      - Chunk 17/58 => 100 contactos OK.\n","      - Chunk 18/58 => 100 contactos OK.\n","      - Chunk 19/58 => 100 contactos OK.\n","      - Chunk 20/58 => 100 contactos OK.\n","      - Chunk 21/58 => 100 contactos OK.\n","      - Chunk 22/58 => 100 contactos OK.\n","      - Chunk 23/58 => 100 contactos OK.\n","      - Chunk 24/58 => 100 contactos OK.\n","      - Chunk 25/58 => 100 contactos OK.\n","      - Chunk 26/58 => 100 contactos OK.\n","      - Chunk 27/58 => 100 contactos OK.\n","      - Chunk 28/58 => 100 contactos OK.\n","      - Chunk 29/58 => 100 contactos OK.\n","      - Chunk 30/58 => 100 contactos OK.\n","      - Chunk 31/58 => 100 contactos OK.\n","      - Chunk 32/58 => 100 contactos OK.\n","      - Chunk 33/58 => 100 contactos OK.\n","      - Chunk 34/58 => 100 contactos OK.\n","      - Chunk 35/58 => 100 contactos OK.\n","      - Chunk 36/58 => 100 contactos OK.\n","      - Chunk 37/58 => 100 contactos OK.\n","      - Chunk 38/58 => 100 contactos OK.\n","      - Chunk 39/58 => 100 contactos OK.\n","      - Chunk 40/58 => 100 contactos OK.\n","      - Chunk 41/58 => 100 contactos OK.\n","      - Chunk 42/58 => 100 contactos OK.\n","      - Chunk 43/58 => 100 contactos OK.\n","      - Chunk 44/58 => 100 contactos OK.\n","      - Chunk 45/58 => 100 contactos OK.\n","      - Chunk 46/58 => 100 contactos OK.\n","      - Chunk 47/58 => 100 contactos OK.\n","      - Chunk 48/58 => 100 contactos OK.\n","      - Chunk 49/58 => 100 contactos OK.\n","      - Chunk 50/58 => 100 contactos OK.\n","      - Chunk 51/58 => 100 contactos OK.\n","      - Chunk 52/58 => 100 contactos OK.\n","      - Chunk 53/58 => 100 contactos OK.\n","      - Chunk 54/58 => 100 contactos OK.\n","      - Chunk 55/58 => 100 contactos OK.\n","      - Chunk 56/58 => 100 contactos OK.\n","      - Chunk 57/58 => 100 contactos OK.\n","      - Chunk 58/58 => 77 contactos OK.\n","   [INFO] Subiendo DataFrame con 5777 filas a BigQuery (disposition=WRITE_APPEND).\n","      - Subiendo CSV a gs://temp_datawarehouse/temp_contacts_c6d7402be75f45c5866ab192c6da240a.csv ...\n","      - Cargando en BQ => animum-dev-datawarehouse.tp_02st_01.hs_contact_sensitive_cleaned ...\n","      ✅ Carga completada. Se subieron 5777 filas.\n","   [INFO] No se alcanzó el límite de 10k. No hay más contactos en el rango.\n","=== Extracción y carga completadas exitosamente. ===\n"]}]}]}