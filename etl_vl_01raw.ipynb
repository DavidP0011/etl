{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidP0011/etl/blob/main/etl_vl_01raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INICIALIZACI√ìN"
      ],
      "metadata": {
        "id": "SV1jWDmAcEBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title INSTALACI√ìN DE LIBRER√çAS\n",
        "!pip install boto3\n",
        "!pip install google-cloud-secret-manager\n",
        "!pip install dateparser\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOXVVtB8cJFu",
        "outputId": "5ef9d383-668c-44d5-844e-268ce7741d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.36.14)\n",
            "Requirement already satisfied: botocore<1.37.0,>=1.36.14 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.36.14)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.11.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.37.0,>=1.36.14->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.37.0,>=1.36.14->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.14->boto3) (1.17.0)\n",
            "Requirement already satisfied: google-cloud-secret-manager in /usr/local/lib/python3.10/dist-packages (2.22.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (2.19.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (4.25.5)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-secret-manager) (0.14.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-secret-manager) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-secret-manager) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-secret-manager) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-secret-manager) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-secret-manager) (2024.12.14)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2024.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.10/dist-packages (from dateparser) (5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title IMPORTACI√ìN DE LIBRER√çAS GCP\n",
        "\n",
        "import boto3\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "from google.cloud import bigquery  # Importa el cliente de BigQuery\n",
        "from google.cloud import storage   # Importa el cliente de Cloud Storage\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import unicodedata\n",
        "import chardet  # Biblioteca para detectar codificaciones de texto\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.cloud import secretmanager\n",
        "import os\n",
        "\n",
        "# Configura el cliente de Secret Manager\n",
        "client = secretmanager.SecretManagerServiceClient()\n",
        "project_id = \"animum-dev-datawarehouse\"\n",
        "\n",
        "# Funci√≥n para obtener un secreto por su nombre\n",
        "def get_secret(secret_id):\n",
        "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/latest\"\n",
        "    response = client.access_secret_version(request={\"name\": name})\n",
        "    return response.payload.data.decode(\"UTF-8\")\n",
        "\n",
        "# Obt√©n los valores de los secretos\n",
        "aws_access_key_id = get_secret(\"AWS_ACCESS_KEY_ID_velneo\")\n",
        "aws_secret_access_key = get_secret(\"AWS_SECRET_ACCESS_KEY_velneo\")\n",
        "\n",
        "# Configura las variables de entorno\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key_id\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_access_key\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'eu-west-1'\n",
        "\n",
        "print(\"Secretos configurados correctamente.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CPamw_5plJ6H",
        "outputId": "cb1a524a-b86d-45c2-9f90-76a6b393f4db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n",
            "Secretos configurados correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title S3_folder_and_files_list()\n",
        "def S3_folder_and_files_list(params: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Lista todos los archivos y subcarpetas a partir de una carpeta espec√≠fica en un bucket de s3.\n",
        "\n",
        "    Args:\n",
        "        params (dict): Diccionario con las claves:\n",
        "            - S3_bucket_name (str): Nombre del bucket de s3.\n",
        "            - S3_folder_path (str): Ruta de la carpeta en el bucket de s3.\n",
        "\n",
        "    Returns:\n",
        "        dict: Diccionario con la estructura de carpetas y archivos:\n",
        "            {\n",
        "                'folders': {\n",
        "                    'subcarpeta1/': {\n",
        "                        'files': [lista de archivos en subcarpeta1],\n",
        "                        'folders': {estructura recursiva de subcarpetas}\n",
        "                    }\n",
        "                },\n",
        "                'files': [lista de archivos en la carpeta actual]\n",
        "            }\n",
        "\n",
        "    Raises:\n",
        "        ValueError: Si falta alg√∫n par√°metro en params.\n",
        "        Exception: Si ocurre un error al listar los objetos.\n",
        "    \"\"\"\n",
        "    import boto3\n",
        "    import logging\n",
        "\n",
        "    # Configurar el registro\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # Validar par√°metros\n",
        "    S3_bucket_name = params.get('S3_bucket_name')\n",
        "    S3_folder_path = params.get('S3_folder_path')\n",
        "\n",
        "    if not S3_bucket_name or not S3_folder_path:\n",
        "        raise ValueError(\"Faltan par√°metros requeridos: 'S3_bucket_name' o 'S3_folder_path'.\")\n",
        "\n",
        "    try:\n",
        "        # Cliente de s3\n",
        "        s3_client = boto3.client('s3')\n",
        "\n",
        "        paginator = s3_client.get_paginator('list_objects_v2')\n",
        "        pages = paginator.paginate(Bucket=S3_bucket_name, Prefix=S3_folder_path)\n",
        "\n",
        "        structure = {'folders': {}, 'files': []}\n",
        "\n",
        "        # Procesar los objetos obtenidos\n",
        "        for page in pages:\n",
        "            for obj in page.get('Contents', []):\n",
        "                key = obj['Key']\n",
        "                if key.endswith('/'):\n",
        "                    # Es una carpeta\n",
        "                    relative_path = key[len(S3_folder_path):]\n",
        "                    if '/' not in relative_path.strip('/'):\n",
        "                        structure['folders'][key] = {\n",
        "                            'files': [],\n",
        "                            'folders': {}\n",
        "                        }\n",
        "                else:\n",
        "                    # Es un archivo\n",
        "                    relative_path = key[len(S3_folder_path):]\n",
        "                    if '/' not in relative_path:\n",
        "                        structure['files'].append(key)\n",
        "\n",
        "        return structure\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al listar objetos en s3: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "ihxdxD65l2xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uww4Tf2JjYsd"
      },
      "outputs": [],
      "source": [
        "# @title S3_to_GCS_transfer_file()\n",
        "def S3_to_GCS_transfer_file(config: dict):\n",
        "    \"\"\"\n",
        "    Transfiere archivos desde un bucket S3 a un bucket de Google Cloud Storage.\n",
        "    Muestra mensajes por pantalla en tiempo real.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Diccionario con las claves:\n",
        "            - S3_bucket_name (str): Nombre del bucket en S3.\n",
        "            - S3_folder_path (str): Ruta de la carpeta en el bucket de S3.\n",
        "            - GCS_bucket_name (str): Nombre del bucket en Google Cloud Storage.\n",
        "            - filter_file (dict): Diccionario con opciones de filtro para los archivos:\n",
        "                - use_bool (bool): Activar o desactivar los filtros.\n",
        "                - name_include_patterns_list (list[str]): Lista de patrones a incluir por nombre.\n",
        "                - name_exclude_patterns_list (list[str]): Lista de patrones a excluir por nombre.\n",
        "                - extension_include_patterns_list (list[str]): Extensiones permitidas.\n",
        "                - extension_exclude_patterns_list (list[str]): Extensiones excluidas.\n",
        "                - min_size_kb (int): Tama√±o m√≠nimo del archivo en KB.\n",
        "                - max_size_kb (int): Tama√±o m√°ximo del archivo en KB.\n",
        "                - modified_after_date (str): Fecha m√≠nima de modificaci√≥n (YYYY-MM-DD).\n",
        "                - modified_before_date (str): Fecha m√°xima de modificaci√≥n (YYYY-MM-DD).\n",
        "                - include_subfolders_bool (bool): Incluir subcarpetas en el filtro.\n",
        "    \"\"\"\n",
        "    import boto3\n",
        "    from google.cloud import storage\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "\n",
        "    start_time = datetime.now()\n",
        "    print(f\"\\n=== Iniciando proceso de transferencia de archivos | {start_time} ===\\n\", flush=True)\n",
        "\n",
        "    # Extraer par√°metros\n",
        "    S3_bucket_name = config['S3_bucket_name']\n",
        "    S3_folder_path = config['S3_folder_path']\n",
        "    GCS_bucket_name = config['GCS_bucket_name']\n",
        "    filter_file = config.get('filter_file', {})\n",
        "\n",
        "    print(\"Autenticando cliente de s3...\", flush=True)\n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    print(\"Autenticando cliente de Google Cloud Storage...\", flush=True)\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(GCS_bucket_name)\n",
        "\n",
        "    # Listar objetos en la carpeta s3\n",
        "    print(f\"Listando archivos en s3 bucket: '{S3_bucket_name}', carpeta: '{S3_folder_path}'\\n\", flush=True)\n",
        "    s3_objects = s3_client.list_objects_v2(Bucket=S3_bucket_name, Prefix=S3_folder_path)\n",
        "\n",
        "    if 'Contents' not in s3_objects:\n",
        "        print(\"No se encontraron objetos en la carpeta s3 especificada.\\n\", flush=True)\n",
        "        return\n",
        "\n",
        "    # Extraer la lista de archivos (claves)\n",
        "    s3_files = []\n",
        "    for obj in s3_objects['Contents']:\n",
        "        key = obj['Key']\n",
        "        # Subcarpetas\n",
        "        if not filter_file.get('include_subfolders_bool', True):\n",
        "            relative_path = key[len(S3_folder_path):]\n",
        "            if '/' in relative_path:\n",
        "                continue\n",
        "        s3_files.append(key)\n",
        "\n",
        "    # Aplicar filtros si est√°n habilitados\n",
        "    if filter_file.get('use_bool', False):\n",
        "        print(\"Aplicando filtros definidos en 'filter_file':\", flush=True)\n",
        "        print(filter_file, flush=True)\n",
        "\n",
        "        # Filtrar por patrones en el nombre\n",
        "        include_patterns = filter_file.get('name_include_patterns_list', [])\n",
        "        exclude_patterns = filter_file.get('name_exclude_patterns_list', [])\n",
        "\n",
        "        if include_patterns:\n",
        "            before_count = len(s3_files)\n",
        "            s3_files = [\n",
        "                file for file in s3_files\n",
        "                if any(pattern in os.path.basename(file) for pattern in include_patterns)\n",
        "            ]\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Filtrando por include_patterns={include_patterns}: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "        if exclude_patterns:\n",
        "            before_count = len(s3_files)\n",
        "            s3_files = [\n",
        "                file for file in s3_files\n",
        "                if not any(pattern in os.path.basename(file) for pattern in exclude_patterns)\n",
        "            ]\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Filtrando por exclude_patterns={exclude_patterns}: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "        # Filtrar por extensi√≥n\n",
        "        include_extensions = filter_file.get('extension_include_patterns_list', [])\n",
        "        exclude_extensions = filter_file.get('extension_exclude_patterns_list', [])\n",
        "\n",
        "        if include_extensions:\n",
        "            before_count = len(s3_files)\n",
        "            s3_files = [file for file in s3_files if any(file.endswith(ext) for ext in include_extensions)]\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Filtrando extensiones permitidas={include_extensions}: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "        if exclude_extensions:\n",
        "            before_count = len(s3_files)\n",
        "            s3_files = [file for file in s3_files if not any(file.endswith(ext) for ext in exclude_extensions)]\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Excluyendo extensiones={exclude_extensions}: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "        # Filtrar por tama√±o\n",
        "        min_size_kb = filter_file.get('min_size_kb')\n",
        "        max_size_kb = filter_file.get('max_size_kb')\n",
        "        if min_size_kb or max_size_kb:\n",
        "            print(f\"Filtrando por tama√±o: min_size_kb={min_size_kb}, max_size_kb={max_size_kb}\", flush=True)\n",
        "            before_count = len(s3_files)\n",
        "            all_objects = {obj['Key']: obj for obj in s3_objects['Contents']}\n",
        "            filtered_files = []\n",
        "            for file in s3_files:\n",
        "                obj = all_objects.get(file)\n",
        "                if not obj:\n",
        "                    continue\n",
        "                file_size_kb = obj['Size'] / 1024\n",
        "                if (min_size_kb and file_size_kb < min_size_kb):\n",
        "                    continue\n",
        "                if (max_size_kb and file_size_kb > max_size_kb):\n",
        "                    continue\n",
        "                filtered_files.append(file)\n",
        "            s3_files = filtered_files\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Filtrando por tama√±o: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "        # Filtrar por fecha de modificaci√≥n\n",
        "        modified_after = filter_file.get('modified_after_date')\n",
        "        modified_before = filter_file.get('modified_before_date')\n",
        "        if modified_after or modified_before:\n",
        "            print(f\"Filtrando por fecha. after={modified_after}, before={modified_before}\", flush=True)\n",
        "            before_count = len(s3_files)\n",
        "            from datetime import datetime\n",
        "            modified_after_dt = datetime.strptime(modified_after, \"%Y-%m-%d\") if modified_after else None\n",
        "            modified_before_dt = datetime.strptime(modified_before, \"%Y-%m-%d\") if modified_before else None\n",
        "\n",
        "            all_objects = {obj['Key']: obj for obj in s3_objects['Contents']}\n",
        "            filtered_files = []\n",
        "            for file in s3_files:\n",
        "                obj = all_objects.get(file)\n",
        "                if not obj:\n",
        "                    continue\n",
        "                last_modified = obj['LastModified']\n",
        "\n",
        "                if modified_after_dt and last_modified < modified_after_dt:\n",
        "                    continue\n",
        "                if modified_before_dt and last_modified > modified_before_dt:\n",
        "                    continue\n",
        "                filtered_files.append(file)\n",
        "\n",
        "            s3_files = filtered_files\n",
        "            after_count = len(s3_files)\n",
        "            print(f\"Filtrado por fecha: {before_count} -> {after_count}\", flush=True)\n",
        "\n",
        "    # Verificar si hay archivos por transferir\n",
        "    if not s3_files:\n",
        "        print(\"No se encontraron archivos para transferir tras aplicar filtros.\\n\", flush=True)\n",
        "        end_time = datetime.now()\n",
        "        print(f\"=== Proceso finalizado sin transferencias | {end_time} ===\\n\", flush=True)\n",
        "        return\n",
        "\n",
        "    print(\"=== Comenzando a transferir archivos ===\\n\", flush=True)\n",
        "    transfer_log = []\n",
        "\n",
        "    for s3_file in s3_files:\n",
        "        temp_file_name = os.path.basename(s3_file)\n",
        "        temp_file_path = f\"/tmp/{temp_file_name}\"\n",
        "\n",
        "        print(f\"Iniciando transferencia de '{s3_file}'\", flush=True)\n",
        "        try:\n",
        "            # Descargar archivo desde s3\n",
        "            print(f\"Descargando: s3://{S3_bucket_name}/{s3_file} -> {temp_file_path}\", flush=True)\n",
        "            s3_client.download_file(S3_bucket_name, s3_file, temp_file_path)\n",
        "            print(f\"Archivo descargado correctamente: {temp_file_path}\", flush=True)\n",
        "\n",
        "            # Subir archivo a GCS\n",
        "            GCS_file_name = temp_file_name\n",
        "            print(f\"Subiendo: {temp_file_path} -> gs://{GCS_bucket_name}/{GCS_file_name}\", flush=True)\n",
        "            blob = bucket.blob(GCS_file_name)\n",
        "            blob.upload_from_filename(temp_file_path)\n",
        "            print(f\"Archivo subido correctamente: gs://{GCS_bucket_name}/{GCS_file_name}\", flush=True)\n",
        "\n",
        "            transfer_log.append({\n",
        "                'archivo_s3': f\"s3://{S3_bucket_name}/{s3_file}\",\n",
        "                'archivo_GCS': f\"gs://{GCS_bucket_name}/{GCS_file_name}\",\n",
        "                'estado': 'Transferido con √©xito'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error al transferir '{s3_file}': {e}\", flush=True)\n",
        "            transfer_log.append({\n",
        "                'archivo_s3': f\"s3://{S3_bucket_name}/{s3_file}\",\n",
        "                'archivo_GCS': None,\n",
        "                'estado': f\"Error: {e}\"\n",
        "            })\n",
        "        finally:\n",
        "            # Eliminar archivo temporal\n",
        "            if os.path.exists(temp_file_path):\n",
        "                os.remove(temp_file_path)\n",
        "                print(f\"Eliminado archivo temporal: {temp_file_path}\\n\", flush=True)\n",
        "\n",
        "    # Resumen final\n",
        "    end_time = datetime.now()\n",
        "    print(f\"=== Proceso de transferencia finalizado | {end_time} ===\", flush=True)\n",
        "    print(f\"Duraci√≥n total: {end_time - start_time}\", flush=True)\n",
        "    print(\"\\n=== Informe de transferencia ===\", flush=True)\n",
        "    for log in transfer_log:\n",
        "        print(f\"- {log['archivo_s3']} -> {log['archivo_GCS']} | Estado: {log['estado']}\", flush=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title GCS_load_CSV_to_GBQ\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import datetime\n",
        "import unicodedata\n",
        "\n",
        "import chardet\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dateparser\n",
        "import pandas_gbq\n",
        "\n",
        "from google.cloud import storage, bigquery\n",
        "\n",
        "def GCS_load_CSV_to_GBQ(params: dict) -> None:\n",
        "    \"\"\"\n",
        "    Funci√≥n unificada que:\n",
        "      1) Carga archivos CSV desde GCS a BigQuery (forzando todas las columnas como STRING)\n",
        "         seg√∫n filtros y nombres indicados.\n",
        "      2) Inferencia de tipos de columnas (bool, int, float, fecha, string) con muestreo.\n",
        "      3) Crea (o reemplaza) la tabla definitiva con tipos correctos, y elimina la tabla temporal.\n",
        "\n",
        "    Par√°metros esperados en 'params':\n",
        "      - project_id (str): ID del proyecto en Google Cloud.\n",
        "      - source_GCS_bucket_name (str): Nombre del bucket de GCS.\n",
        "      - source_GCS_file_names_list (list[str]): Lista de nombres de archivos CSV.\n",
        "        Si est√° vac√≠o, se importan todos los del bucket (aplicando filtros).\n",
        "      - source_GCS_file_names_filter (dict): Opciones de filtro para los archivos:\n",
        "          - use_bool (bool): Activar/desactivar filtros.\n",
        "          - name_include_patterns_list (list[str]): Patrones a incluir por nombre.\n",
        "          - name_exclude_patterns_list (list[str]): Patrones a excluir por nombre.\n",
        "          - extension_include_patterns_list (list[str]): Extensiones permitidas.\n",
        "          - extension_exclude_patterns_list (list[str]): Extensiones excluidas.\n",
        "          - min_size_kb (int): Tama√±o m√≠nimo (KB).\n",
        "          - max_size_kb (int): Tama√±o m√°ximo (KB).\n",
        "          - modified_after_date (str): Fecha m√≠nima de modificaci√≥n (YYYY-MM-DD).\n",
        "          - modified_before_date (str): Fecha m√°xima de modificaci√≥n (YYYY-MM-DD).\n",
        "          - include_subfolders_bool (bool): Incluir subcarpetas.\n",
        "      - destination_GBQ_dataset_id (str): ID del dataset de BigQuery.\n",
        "      - reemplazos (dict): Diccionario de reemplazos en el nombre final de la tabla.\n",
        "        (ejemplo: {\"-utf8\": \"\"})\n",
        "\n",
        "      -- Par√°metros de inferencia de tipos:\n",
        "      - inference_limit (int): n√∫mero de filas a muestrear para la inferencia (por defecto 1000).\n",
        "      - inference_threshold (float): % m√≠nimo de filas no nulas que deben cumplir la condici√≥n\n",
        "                                     para considerar ese tipo (por defecto 0.95).\n",
        "\n",
        "    Comportamiento:\n",
        "      - Se cargan los CSV con todas sus columnas como STRING a una tabla temporal <tabla>__TMP.\n",
        "      - Se infieren tipos: BOOL, INT64, FLOAT64, TIMESTAMP, STRING.\n",
        "        Adem√°s, si en el nombre de columna existe \"fecha\" o \"date\", se fuerza a TIMESTAMP/DATE.\n",
        "      - Se crea (o reemplaza) la tabla final <tabla> con CASTs apropiados.\n",
        "      - Se elimina la tabla temporal <tabla>__TMP.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #\n",
        "    # ============= 1) Definici√≥n de sub-funciones auxiliares =============\n",
        "    #\n",
        "\n",
        "    def normalize_column_name(name: str) -> str:\n",
        "        \"\"\"Normaliza un nombre para BigQuery: ASCII, guiones bajos, m√°x 300 chars.\"\"\"\n",
        "        name = unicodedata.normalize('NFKD', name).encode('ASCII', 'ignore').decode('utf-8')\n",
        "        name = name.replace('√±', 'n').replace('√ë', 'N')\n",
        "        normalized = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", name)\n",
        "        normalized = re.sub(r\"_+\", \"_\", normalized).strip(\"_\")\n",
        "        return normalized[:300]\n",
        "\n",
        "    def file_passes_filters(blob) -> bool:\n",
        "        \"\"\"Determina si un archivo cumple los filtros especificados en 'source_GCS_file_names_filter'.\"\"\"\n",
        "        if not filter_file.get(\"use_bool\", False):\n",
        "            return True\n",
        "\n",
        "        blob_name = blob.name\n",
        "        # Verificar patrones de nombre (include / exclude)\n",
        "        if filter_file.get(\"name_include_patterns_list\") and not any(pat in blob_name for pat in filter_file[\"name_include_patterns_list\"]):\n",
        "            return False\n",
        "        if filter_file.get(\"name_exclude_patterns_list\") and any(pat in blob_name for pat in filter_file[\"name_exclude_patterns_list\"]):\n",
        "            return False\n",
        "\n",
        "        # Verificar extensiones\n",
        "        extension = os.path.splitext(blob_name)[1]\n",
        "        if filter_file.get(\"extension_include_patterns_list\") and extension not in filter_file[\"extension_include_patterns_list\"]:\n",
        "            return False\n",
        "        if filter_file.get(\"extension_exclude_patterns_list\") and extension in filter_file[\"extension_exclude_patterns_list\"]:\n",
        "            return False\n",
        "\n",
        "        # Verificar tama√±o\n",
        "        size_kb = blob.size / 1024\n",
        "        if filter_file.get(\"min_size_kb\") and size_kb < filter_file[\"min_size_kb\"]:\n",
        "            return False\n",
        "        if filter_file.get(\"max_size_kb\") and size_kb > filter_file[\"max_size_kb\"]:\n",
        "            return False\n",
        "\n",
        "        # Verificar fechas de modificaci√≥n\n",
        "        modified_date = blob.updated.date() if blob.updated else None\n",
        "        if modified_date:\n",
        "            if filter_file.get(\"modified_after_date\"):\n",
        "                after_date = datetime.datetime.strptime(filter_file[\"modified_after_date\"], \"%Y-%m-%d\").date()\n",
        "                if modified_date < after_date:\n",
        "                    return False\n",
        "            if filter_file.get(\"modified_before_date\"):\n",
        "                before_date = datetime.datetime.strptime(filter_file[\"modified_before_date\"], \"%Y-%m-%d\").date()\n",
        "                if modified_date > before_date:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def detect_encoding_and_load_df(blob):\n",
        "        \"\"\"Descarga el blob, detecta la codificaci√≥n y retorna un DataFrame (asumiendo separador ';').\"\"\"\n",
        "        raw_data = blob.download_as_bytes()\n",
        "        encoding_detected = chardet.detect(raw_data)['encoding']\n",
        "        if not encoding_detected:\n",
        "            encoding_detected = 'utf-8'  # fallback\n",
        "\n",
        "        raw_text = raw_data.decode(encoding_detected, errors=\"replace\")\n",
        "        df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n",
        "        return df\n",
        "\n",
        "    #\n",
        "    # ============= 2) Inferencia de tipos con muestreo =============\n",
        "    #\n",
        "    # Mismos helpers de tu segundo script, adaptados dentro de la funci√≥n principal.\n",
        "\n",
        "    cache_fecha = {}  # Cache para almacenar resultados de parseo.\n",
        "\n",
        "    def es_fecha(valor):\n",
        "        val_str = str(valor).strip()\n",
        "        if not any(ch.isdigit() for ch in val_str):  # si no hay d√≠gitos, dif√≠cilmente sea fecha\n",
        "            return False\n",
        "        if val_str in cache_fecha:\n",
        "            return cache_fecha[val_str]\n",
        "        try:\n",
        "            parsed = dateparser.parse(val_str, languages=['en', 'es'])\n",
        "            result = parsed is not None\n",
        "            cache_fecha[val_str] = result\n",
        "            return result\n",
        "        except Exception:\n",
        "            cache_fecha[val_str] = False\n",
        "            return False\n",
        "\n",
        "    def es_booleano(valor):\n",
        "        if isinstance(valor, bool):\n",
        "            return True\n",
        "        if isinstance(valor, (int, float)):\n",
        "            return valor in [0, 1]\n",
        "        if isinstance(valor, str):\n",
        "            v = valor.strip().lower()\n",
        "            return v in {\"true\", \"false\", \"yes\", \"no\", \"si\", \"0\", \"1\"}\n",
        "        return False\n",
        "\n",
        "    def normalizar_bool(valor):\n",
        "        if isinstance(valor, bool):\n",
        "            return valor\n",
        "        if isinstance(valor, (int, float)):\n",
        "            return valor == 1\n",
        "        if isinstance(valor, str):\n",
        "            v = valor.strip().lower()\n",
        "            if v in {\"true\", \"yes\", \"si\", \"1\"}:\n",
        "                return True\n",
        "            elif v in {\"false\", \"no\", \"0\"}:\n",
        "                return False\n",
        "        return None\n",
        "\n",
        "    def es_entero(valor):\n",
        "        try:\n",
        "            if isinstance(valor, (int, np.integer)):\n",
        "                return True\n",
        "            if isinstance(valor, float) and valor.is_integer():\n",
        "                return True\n",
        "            int(str(valor))\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def es_flotante(valor):\n",
        "        try:\n",
        "            float(valor)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def inferir_tipo(serie, threshold):\n",
        "        datos = serie.dropna()\n",
        "        if datos.empty:\n",
        "            return \"STRING\"\n",
        "        total = len(datos)\n",
        "        # Verificar booleana\n",
        "        bool_count = datos.apply(es_booleano).sum()\n",
        "        if (bool_count / total) >= threshold:\n",
        "            bool_values = datos[datos.apply(es_booleano)].apply(normalizar_bool)\n",
        "            if bool_values.nunique() <= 2:\n",
        "                return \"BOOLEAN\"\n",
        "        # Verificar entero\n",
        "        if (datos.apply(es_entero).sum() / total) >= threshold:\n",
        "            return \"INTEGER\"\n",
        "        # Verificar float\n",
        "        if (datos.apply(es_flotante).sum() / total) >= threshold:\n",
        "            return \"FLOAT\"\n",
        "        # Verificar fecha\n",
        "        if (datos.apply(es_fecha).sum() / total) >= threshold:\n",
        "            return \"TIMESTAMP\"\n",
        "        return \"STRING\"\n",
        "\n",
        "    # Mapeo final de tipos a BigQuery\n",
        "    type_mapping = {\n",
        "        \"BOOLEAN\": \"BOOL\",\n",
        "        \"INTEGER\": \"INT64\",\n",
        "        \"FLOAT\": \"FLOAT64\",\n",
        "        \"TIMESTAMP\": \"TIMESTAMP\",\n",
        "        \"STRING\": \"STRING\"\n",
        "    }\n",
        "\n",
        "    #\n",
        "    # ============= 3) Lectura de par√°metros y set-up =============\n",
        "    #\n",
        "\n",
        "    # Extraer y validar par√°metros\n",
        "    project_id = params.get(\"project_id\")\n",
        "    bucket_name = params.get(\"source_GCS_bucket_name\")\n",
        "    file_names = params.get(\"source_GCS_file_names_list\", [])\n",
        "    dataset_id = params.get(\"destination_GBQ_dataset_id\")\n",
        "    filter_file = params.get(\"source_GCS_file_names_filter\", {})\n",
        "    reemplazos = params.get(\"reemplazos\", {})\n",
        "\n",
        "    inference_limit = params.get(\"inference_limit\", 1000)\n",
        "    inference_threshold = params.get(\"inference_threshold\", 0.95)\n",
        "\n",
        "    if not all([project_id, bucket_name, dataset_id]):\n",
        "        raise ValueError(\"‚ùå Faltan par√°metros obligatorios: project_id, source_GCS_bucket_name, destination_GBQ_dataset_id.\")\n",
        "\n",
        "    storage_client = storage.Client(project=project_id)\n",
        "    bq_client = bigquery.Client(project=project_id)\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = list(bucket.list_blobs(prefix=\"\", delimiter=\"/\" if not filter_file.get(\"include_subfolders_bool\", False) else None))\n",
        "\n",
        "    # Si no se especifican nombres, se procesan todos los archivos filtrados\n",
        "    if not file_names:\n",
        "        candidate_files = [blob.name for blob in blobs if file_passes_filters(blob)]\n",
        "    else:\n",
        "        # De la lista, quedarnos solo con los que pasan filtros\n",
        "        candidate_files = []\n",
        "        for blob in blobs:\n",
        "            if blob.name in file_names and file_passes_filters(blob):\n",
        "                candidate_files.append(blob.name)\n",
        "\n",
        "    if not candidate_files:\n",
        "        print(\"No se encontraron archivos que cumplan los filtros/nombres especificados.\")\n",
        "        return\n",
        "\n",
        "    #\n",
        "    # ============= 4) Proceso principal para cada archivo =============\n",
        "    #\n",
        "    for file_name in candidate_files:\n",
        "        print(f\"\\n=== Procesando archivo: {file_name} ===\")\n",
        "        blob = bucket.blob(file_name)\n",
        "\n",
        "        if not blob.exists():\n",
        "            print(f\"‚ùå El archivo '{file_name}' no existe en el bucket '{bucket_name}'.\")\n",
        "            continue\n",
        "\n",
        "        # 4.1) Descargar y convertir a DataFrame\n",
        "        try:\n",
        "            df = detect_encoding_and_load_df(blob)\n",
        "            print(f\"‚úîÔ∏è Archivo '{file_name}' cargado en DataFrame (columnas: {len(df.columns)})\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al leer el archivo '{file_name}': {e}\")\n",
        "            continue\n",
        "\n",
        "        # 4.2) Normalizar nombres de columnas\n",
        "        original_columns = df.columns.tolist()\n",
        "        normalized_columns = [normalize_column_name(col) for col in original_columns]\n",
        "        df.columns = normalized_columns\n",
        "\n",
        "        # 4.3) Nombre base de la tabla en BigQuery (aplicando reemplazos)\n",
        "        raw_table_name = os.path.splitext(file_name)[0]\n",
        "        for old, new in reemplazos.items():\n",
        "            raw_table_name = raw_table_name.replace(old, new)\n",
        "        raw_table_name = raw_table_name.replace(\"-\", \"_\")\n",
        "        final_table_name = normalize_column_name(raw_table_name)\n",
        "\n",
        "        # 4.4) Definimos la tabla temporal y la tabla final\n",
        "        tmp_table_id = f\"{project_id}.{dataset_id}.{final_table_name}__TMP\"\n",
        "        final_table_id = f\"{project_id}.{dataset_id}.{final_table_name}\"\n",
        "\n",
        "        # 4.5) Subir a BigQuery la tabla temporal, forzando schema de tipo STRING\n",
        "        #     (Creamos un schema de BigQuery con todas las columnas como STRING,\n",
        "        #      pero con \"description\" la columna original, si se quiere.)\n",
        "        schema = [\n",
        "            bigquery.SchemaField(col, \"STRING\", description=f\"Original: {original_columns[i]}\")\n",
        "            for i, col in enumerate(normalized_columns)\n",
        "        ]\n",
        "\n",
        "        # Guardar DataFrame en CSV (UTF-8) temporal para cargarlo\n",
        "        local_tmp_file = f\"/tmp/{final_table_name}__TMP.csv\"\n",
        "        df.to_csv(local_tmp_file, index=False, sep=\";\", encoding=\"utf-8\")\n",
        "\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            source_format=bigquery.SourceFormat.CSV,\n",
        "            skip_leading_rows=1,\n",
        "            field_delimiter=\";\",\n",
        "            quote_character='\"',\n",
        "            allow_quoted_newlines=True,\n",
        "            encoding=\"UTF-8\",\n",
        "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "            schema=schema,\n",
        "            max_bad_records=10\n",
        "        )\n",
        "\n",
        "        print(f\"üì§ Cargando datos a la tabla temporal '{tmp_table_id}'...\")\n",
        "        try:\n",
        "            with open(local_tmp_file, \"rb\") as file_obj:\n",
        "                load_job = bq_client.load_table_from_file(file_obj, tmp_table_id, job_config=job_config)\n",
        "            load_job.result()\n",
        "            print(f\"‚úîÔ∏è Carga finalizada en la tabla temporal: {tmp_table_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al cargar la tabla temporal '{tmp_table_id}': {e}\")\n",
        "            continue\n",
        "\n",
        "        # 4.6) Inferir tipos tomando una muestra de la tabla temporal\n",
        "        #      Extraemos la muestra con un SELECT * LIMIT inference_limit\n",
        "        print(\"üîé Inferencia de tipos en la tabla temporal (muestreo)...\")\n",
        "        sample_query = f\"\"\"\n",
        "            SELECT *\n",
        "            FROM `{tmp_table_id}`\n",
        "            LIMIT {inference_limit}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            sample_df = pandas_gbq.read_gbq(sample_query, project_id=project_id, progress_bar_type=None)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al leer la muestra para inferencia en '{tmp_table_id}': {e}\")\n",
        "            # Opcionalmente: drop la tabla temporal\n",
        "            bq_client.delete_table(tmp_table_id, not_found_ok=True)\n",
        "            continue\n",
        "\n",
        "        esquema_inferido = {}\n",
        "        for col in sample_df.columns:\n",
        "            # Forzamos fecha si el nombre de la columna contiene \"fecha\" o \"date\"\n",
        "            if \"fecha\" in col.lower() or \"date\" in col.lower():\n",
        "                esquema_inferido[col] = \"TIMESTAMP\"\n",
        "            else:\n",
        "                esquema_inferido[col] = inferir_tipo(sample_df[col], inference_threshold)\n",
        "\n",
        "        # 4.7) Construir el SELECT con los SAFE_CAST\n",
        "        select_expressions = []\n",
        "        for col in sample_df.columns:\n",
        "            tipo_inferido = esquema_inferido[col]\n",
        "            bq_type = type_mapping.get(tipo_inferido, \"STRING\")\n",
        "            # Importante: el col puede tener caracteres que requieran backticks\n",
        "            expr = f\"SAFE_CAST(`{col}` AS {bq_type}) AS `{col}`\"\n",
        "            select_expressions.append(expr)\n",
        "        select_clause = \",\\n  \".join(select_expressions)\n",
        "\n",
        "        # 4.8) Creamos/Reemplazamos la tabla final con el nuevo esquema\n",
        "        create_final_sql = f\"\"\"\n",
        "            CREATE OR REPLACE TABLE `{final_table_id}` AS\n",
        "            SELECT\n",
        "              {select_clause}\n",
        "            FROM `{tmp_table_id}`;\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"üì§ Creando/Reemplazando la tabla final '{final_table_id}' con tipos inferidos...\")\n",
        "        try:\n",
        "            bq_client.query(create_final_sql).result()\n",
        "            print(f\"‚úîÔ∏è Tabla final '{final_table_id}' creada con exito.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error al crear la tabla final '{final_table_id}': {e}\")\n",
        "            # en caso de error, no borramos la tabla temporal por si se quiere inspeccionar\n",
        "            continue\n",
        "\n",
        "        # 4.9) Eliminar la tabla temporal\n",
        "        try:\n",
        "            bq_client.delete_table(tmp_table_id, not_found_ok=True)\n",
        "            print(f\"üóëÔ∏è Tabla temporal '{tmp_table_id}' eliminada.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Aviso: no se pudo eliminar la tabla temporal '{tmp_table_id}': {e}\")\n",
        "\n",
        "    print(\"\\n=== Proceso completo finalizado ===\")\n"
      ],
      "metadata": {
        "id": "DBiZbrnB_EZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJECUCIONES"
      ],
      "metadata": {
        "id": "SDY2ZxTa_G-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LISTADO DE ARCHIVOS S3\n",
        "S3_bucket_name = \"animum-datalake-landing-euwest3-637423635409\" # @param {\"type\":\"string\"}\n",
        "S3_folder_path = \"velneo/\" # @param {\"type\":\"string\"}\n",
        "params = {\n",
        "    'S3_bucket_name': S3_bucket_name,\n",
        "    'S3_folder_path': S3_folder_path\n",
        "}\n",
        "\n",
        "# Ejecutar la funci√≥n\n",
        "result = S3_folder_and_files_list(params)\n",
        "\n",
        "import pprint\n",
        "\n",
        "# Mostrar resultado\n",
        "pprint.pprint(result)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dr1gim3zmAnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b1505ce-077c-42c6-dd5d-d05a1b2bdce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'files': ['velneo/ESC-ANOS-utf8.csv',\n",
            "           'velneo/ESC-ANOS.csv',\n",
            "           'velneo/ESC-APET-utf8.csv',\n",
            "           'velneo/ESC-APET.csv',\n",
            "           'velneo/ESC-AULA-utf8.csv',\n",
            "           'velneo/ESC-AULA.csv',\n",
            "           'velneo/ESC-C&M-utf8.csv',\n",
            "           'velneo/ESC-C&M.csv',\n",
            "           'velneo/ESC-C&P-utf8.csv',\n",
            "           'velneo/ESC-C&P.csv',\n",
            "           'velneo/ESC-CALE-utf8.csv',\n",
            "           'velneo/ESC-CALE.csv',\n",
            "           'velneo/ESC-COBR-utf8.csv',\n",
            "           'velneo/ESC-COBR.csv',\n",
            "           'velneo/ESC-CONT-utf8.csv',\n",
            "           'velneo/ESC-CONT.csv',\n",
            "           'velneo/ESC-CONV-utf8.csv',\n",
            "           'velneo/ESC-CONV.csv',\n",
            "           'velneo/ESC-CURS-utf8.csv',\n",
            "           'velneo/ESC-CURS.csv',\n",
            "           'velneo/ESC-DIAS-utf8.csv',\n",
            "           'velneo/ESC-DIAS.csv',\n",
            "           'velneo/ESC-DISC-utf8.csv',\n",
            "           'velneo/ESC-DISC.csv',\n",
            "           'velneo/ESC-EVAC-utf8.csv',\n",
            "           'velneo/ESC-EVAC.csv',\n",
            "           'velneo/ESC-EVAL-utf8.csv',\n",
            "           'velneo/ESC-EVAL.csv',\n",
            "           'velneo/ESC-EVEN-utf8.csv',\n",
            "           'velneo/ESC-EVEN.csv',\n",
            "           'velneo/ESC-EXPE-utf8.csv',\n",
            "           'velneo/ESC-EXPE.csv',\n",
            "           'velneo/ESC-EXPN-utf8.csv',\n",
            "           'velneo/ESC-EXPN.csv',\n",
            "           'velneo/ESC-FACL-utf8.csv',\n",
            "           'velneo/ESC-FACL.csv',\n",
            "           'velneo/ESC-FACT-utf8.csv',\n",
            "           'velneo/ESC-FACT.csv',\n",
            "           'velneo/ESC-GRCC-utf8.csv',\n",
            "           'velneo/ESC-GRCC.csv',\n",
            "           'velneo/ESC-GRCL-utf8.csv',\n",
            "           'velneo/ESC-GRCL.csv',\n",
            "           'velneo/ESC-GRCO-utf8.csv',\n",
            "           'velneo/ESC-GRCO.csv',\n",
            "           'velneo/ESC-GRUP-utf8.csv',\n",
            "           'velneo/ESC-GRUP.csv',\n",
            "           'velneo/ESC-L&T-utf8.csv',\n",
            "           'velneo/ESC-L&T.csv',\n",
            "           'velneo/ESC-LECC-utf8.csv',\n",
            "           'velneo/ESC-LECC.csv',\n",
            "           'velneo/ESC-MATC-utf8.csv',\n",
            "           'velneo/ESC-MATC.csv',\n",
            "           'velneo/ESC-MATL-utf8.csv',\n",
            "           'velneo/ESC-MATL.csv',\n",
            "           'velneo/ESC-MESE-utf8.csv',\n",
            "           'velneo/ESC-MESE.csv',\n",
            "           'velneo/ESC-MODU-utf8.csv',\n",
            "           'velneo/ESC-MODU.csv',\n",
            "           'velneo/ESC-MOTI-utf8.csv',\n",
            "           'velneo/ESC-MOTI.csv',\n",
            "           'velneo/ESC-PAGC-utf8.csv',\n",
            "           'velneo/ESC-PAGC.csv',\n",
            "           'velneo/ESC-PAGD-utf8.csv',\n",
            "           'velneo/ESC-PAGD.csv',\n",
            "           'velneo/ESC-PAGL-utf8.csv',\n",
            "           'velneo/ESC-PAGL.csv',\n",
            "           'velneo/ESC-REPA-utf8.csv',\n",
            "           'velneo/ESC-REPA.csv',\n",
            "           'velneo/ESC-SERI-utf8.csv',\n",
            "           'velneo/ESC-SERI.csv',\n",
            "           'velneo/ESC-SERL-utf8.csv',\n",
            "           'velneo/ESC-SERL.csv',\n",
            "           'velneo/ESC-SESI-utf8.csv',\n",
            "           'velneo/ESC-SESI.csv',\n",
            "           'velneo/ESC-TALL-utf8.csv',\n",
            "           'velneo/ESC-TALL.csv',\n",
            "           'velneo/ESC-TARC-utf8.csv',\n",
            "           'velneo/ESC-TARC.csv',\n",
            "           'velneo/ESC-TARE-utf8.csv',\n",
            "           'velneo/ESC-TARE.csv',\n",
            "           'velneo/ESC-TARL-utf8.csv',\n",
            "           'velneo/ESC-TARL.csv',\n",
            "           'velneo/ESC-TITL-utf8.csv',\n",
            "           'velneo/ESC-TITL.csv',\n",
            "           'velneo/ESC-TITN-utf8.csv',\n",
            "           'velneo/ESC-TITN.csv',\n",
            "           'velneo/ESC-TITU-utf8.csv',\n",
            "           'velneo/ESC-TITU.csv',\n",
            "           'velneo/ESC-TURN-utf8.csv',\n",
            "           'velneo/ESC-TURN.csv',\n",
            "           'velneo/MA-DIRECCIONES-utf8.csv',\n",
            "           'velneo/MA-DIRECCIONES.csv',\n",
            "           'velneo/MA-ENINC-utf8.csv',\n",
            "           'velneo/MA-ENINC.csv',\n",
            "           'velneo/MA-ENTIDADES-utf8.csv',\n",
            "           'velneo/MA-ENTIDADES.csv',\n",
            "           'velneo/MA-FPAGO-utf8.csv',\n",
            "           'velneo/MA-FPAGO.csv',\n",
            "           'velneo/MA-PAISES-utf8.csv',\n",
            "           'velneo/MA-PAISES.csv',\n",
            "           'velneo/MA-PROVINCIAS-utf8.csv',\n",
            "           'velneo/MA-PROVINCIAS.csv'],\n",
            " 'folders': {'velneo/': {'files': [], 'folders': {}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title COPIADO DE ARCHIVOS S3 A GCS\n",
        "\n",
        "S3_bucket_name = \"animum-datalake-landing-euwest3-637423635409\"  # @param {\"type\":\"string\"}\n",
        "S3_folder_path = \"velneo/\"  # @param {\"type\":\"string\"}\n",
        "GCS_bucket_name = \"vl_00csv_01\"  # @param {\"type\":\"string\"}\n",
        "\n",
        "use_bool = True  # @param {\"type\": \"boolean\"}\n",
        "name_include_patterns_list = [\"utf8\"]  # @param {\"type\": \"raw\"}\n",
        "name_exclude_patterns_list = [\"backup\"]  # @param {\"type\": \"raw\"}\n",
        "extension_include_patterns_list = [\".txt\", \".csv\"]  # @param {\"type\": \"raw\"}\n",
        "extension_exclude_patterns_list = [\".log\"]  # @param {\"type\": \"raw\"}\n",
        "min_size_kb = None  # @param {\"type\": \"number\"}\n",
        "max_size_kb = None  # @param {\"type\": \"number\"}\n",
        "modified_before_date = \"\"  # @param {\"type\": \"date\"}\n",
        "modified_after_date = \"\"  # @param {\"type\":\"date\"}\n",
        "include_subfolders_bool = True  # @param {\"type\": \"boolean\"}\n",
        "\n",
        "filter_file = {\n",
        "    \"use_bool\": use_bool,\n",
        "    \"name_include_patterns_list\": name_include_patterns_list,\n",
        "    \"name_exclude_patterns_list\": name_exclude_patterns_list,\n",
        "    \"extension_include_patterns_list\": extension_include_patterns_list,\n",
        "    \"extension_exclude_patterns_list\": extension_exclude_patterns_list,\n",
        "    \"min_size_kb\": min_size_kb,\n",
        "    \"max_size_kb\": max_size_kb,\n",
        "    \"modified_after_date\": modified_after_date,\n",
        "    \"modified_before_date\": modified_before_date,\n",
        "    \"include_subfolders_bool\": include_subfolders_bool,\n",
        "}\n",
        "\n",
        "config = {\n",
        "    \"S3_bucket_name\": S3_bucket_name,\n",
        "    \"S3_folder_path\": S3_folder_path,\n",
        "    \"GCS_bucket_name\": GCS_bucket_name,\n",
        "    \"filter_file\": filter_file,\n",
        "}\n",
        "\n",
        "# Ejecutar\n",
        "S3_to_GCS_transfer_file(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e29A71o6jgav",
        "outputId": "556af626-7482-4437-ddbf-b8112850cb78",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Iniciando proceso de transferencia de archivos | 2025-02-10 08:32:18.669066 ===\n",
            "\n",
            "Autenticando cliente de s3...\n",
            "Autenticando cliente de Google Cloud Storage...\n",
            "Listando archivos en s3 bucket: 'animum-datalake-landing-euwest3-637423635409', carpeta: 'velneo/'\n",
            "\n",
            "Aplicando filtros definidos en 'filter_file':\n",
            "{'use_bool': True, 'name_include_patterns_list': ['utf8'], 'name_exclude_patterns_list': ['backup'], 'extension_include_patterns_list': ['.txt', '.csv'], 'extension_exclude_patterns_list': ['.log'], 'min_size_kb': None, 'max_size_kb': None, 'modified_after_date': '', 'modified_before_date': '', 'include_subfolders_bool': True}\n",
            "Filtrando por include_patterns=['utf8']: 103 -> 51\n",
            "Filtrando por exclude_patterns=['backup']: 51 -> 51\n",
            "Filtrando extensiones permitidas=['.txt', '.csv']: 51 -> 51\n",
            "Excluyendo extensiones=['.log']: 51 -> 51\n",
            "=== Comenzando a transferir archivos ===\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-ANOS-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-ANOS-utf8.csv -> /tmp/ESC-ANOS-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-ANOS-utf8.csv\n",
            "Subiendo: /tmp/ESC-ANOS-utf8.csv -> gs://vl_00csv_01/ESC-ANOS-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-ANOS-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-ANOS-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-APET-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-APET-utf8.csv -> /tmp/ESC-APET-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-APET-utf8.csv\n",
            "Subiendo: /tmp/ESC-APET-utf8.csv -> gs://vl_00csv_01/ESC-APET-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-APET-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-APET-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-AULA-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-AULA-utf8.csv -> /tmp/ESC-AULA-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-AULA-utf8.csv\n",
            "Subiendo: /tmp/ESC-AULA-utf8.csv -> gs://vl_00csv_01/ESC-AULA-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-AULA-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-AULA-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-C&M-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-C&M-utf8.csv -> /tmp/ESC-C&M-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-C&M-utf8.csv\n",
            "Subiendo: /tmp/ESC-C&M-utf8.csv -> gs://vl_00csv_01/ESC-C&M-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-C&M-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-C&M-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-C&P-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-C&P-utf8.csv -> /tmp/ESC-C&P-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-C&P-utf8.csv\n",
            "Subiendo: /tmp/ESC-C&P-utf8.csv -> gs://vl_00csv_01/ESC-C&P-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-C&P-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-C&P-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-CALE-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CALE-utf8.csv -> /tmp/ESC-CALE-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-CALE-utf8.csv\n",
            "Subiendo: /tmp/ESC-CALE-utf8.csv -> gs://vl_00csv_01/ESC-CALE-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-CALE-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-CALE-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-COBR-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-COBR-utf8.csv -> /tmp/ESC-COBR-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-COBR-utf8.csv\n",
            "Subiendo: /tmp/ESC-COBR-utf8.csv -> gs://vl_00csv_01/ESC-COBR-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-COBR-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-COBR-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-CONT-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CONT-utf8.csv -> /tmp/ESC-CONT-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-CONT-utf8.csv\n",
            "Subiendo: /tmp/ESC-CONT-utf8.csv -> gs://vl_00csv_01/ESC-CONT-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-CONT-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-CONT-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-CONV-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CONV-utf8.csv -> /tmp/ESC-CONV-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-CONV-utf8.csv\n",
            "Subiendo: /tmp/ESC-CONV-utf8.csv -> gs://vl_00csv_01/ESC-CONV-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-CONV-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-CONV-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-CURS-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CURS-utf8.csv -> /tmp/ESC-CURS-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-CURS-utf8.csv\n",
            "Subiendo: /tmp/ESC-CURS-utf8.csv -> gs://vl_00csv_01/ESC-CURS-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-CURS-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-CURS-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-DIAS-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-DIAS-utf8.csv -> /tmp/ESC-DIAS-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-DIAS-utf8.csv\n",
            "Subiendo: /tmp/ESC-DIAS-utf8.csv -> gs://vl_00csv_01/ESC-DIAS-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-DIAS-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-DIAS-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-DISC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-DISC-utf8.csv -> /tmp/ESC-DISC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-DISC-utf8.csv\n",
            "Subiendo: /tmp/ESC-DISC-utf8.csv -> gs://vl_00csv_01/ESC-DISC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-DISC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-DISC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-EVAC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVAC-utf8.csv -> /tmp/ESC-EVAC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-EVAC-utf8.csv\n",
            "Subiendo: /tmp/ESC-EVAC-utf8.csv -> gs://vl_00csv_01/ESC-EVAC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-EVAC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-EVAC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-EVAL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVAL-utf8.csv -> /tmp/ESC-EVAL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-EVAL-utf8.csv\n",
            "Subiendo: /tmp/ESC-EVAL-utf8.csv -> gs://vl_00csv_01/ESC-EVAL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-EVAL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-EVAL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-EVEN-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVEN-utf8.csv -> /tmp/ESC-EVEN-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-EVEN-utf8.csv\n",
            "Subiendo: /tmp/ESC-EVEN-utf8.csv -> gs://vl_00csv_01/ESC-EVEN-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-EVEN-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-EVEN-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-EXPE-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EXPE-utf8.csv -> /tmp/ESC-EXPE-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-EXPE-utf8.csv\n",
            "Subiendo: /tmp/ESC-EXPE-utf8.csv -> gs://vl_00csv_01/ESC-EXPE-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-EXPE-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-EXPE-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-EXPN-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EXPN-utf8.csv -> /tmp/ESC-EXPN-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-EXPN-utf8.csv\n",
            "Subiendo: /tmp/ESC-EXPN-utf8.csv -> gs://vl_00csv_01/ESC-EXPN-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-EXPN-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-EXPN-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-FACL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-FACL-utf8.csv -> /tmp/ESC-FACL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-FACL-utf8.csv\n",
            "Subiendo: /tmp/ESC-FACL-utf8.csv -> gs://vl_00csv_01/ESC-FACL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-FACL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-FACL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-FACT-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-FACT-utf8.csv -> /tmp/ESC-FACT-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-FACT-utf8.csv\n",
            "Subiendo: /tmp/ESC-FACT-utf8.csv -> gs://vl_00csv_01/ESC-FACT-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-FACT-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-FACT-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-GRCC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCC-utf8.csv -> /tmp/ESC-GRCC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-GRCC-utf8.csv\n",
            "Subiendo: /tmp/ESC-GRCC-utf8.csv -> gs://vl_00csv_01/ESC-GRCC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-GRCC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-GRCC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-GRCL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCL-utf8.csv -> /tmp/ESC-GRCL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-GRCL-utf8.csv\n",
            "Subiendo: /tmp/ESC-GRCL-utf8.csv -> gs://vl_00csv_01/ESC-GRCL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-GRCL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-GRCL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-GRCO-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCO-utf8.csv -> /tmp/ESC-GRCO-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-GRCO-utf8.csv\n",
            "Subiendo: /tmp/ESC-GRCO-utf8.csv -> gs://vl_00csv_01/ESC-GRCO-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-GRCO-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-GRCO-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-GRUP-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRUP-utf8.csv -> /tmp/ESC-GRUP-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-GRUP-utf8.csv\n",
            "Subiendo: /tmp/ESC-GRUP-utf8.csv -> gs://vl_00csv_01/ESC-GRUP-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-GRUP-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-GRUP-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-L&T-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-L&T-utf8.csv -> /tmp/ESC-L&T-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-L&T-utf8.csv\n",
            "Subiendo: /tmp/ESC-L&T-utf8.csv -> gs://vl_00csv_01/ESC-L&T-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-L&T-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-L&T-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-LECC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-LECC-utf8.csv -> /tmp/ESC-LECC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-LECC-utf8.csv\n",
            "Subiendo: /tmp/ESC-LECC-utf8.csv -> gs://vl_00csv_01/ESC-LECC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-LECC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-LECC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-MATC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MATC-utf8.csv -> /tmp/ESC-MATC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-MATC-utf8.csv\n",
            "Subiendo: /tmp/ESC-MATC-utf8.csv -> gs://vl_00csv_01/ESC-MATC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-MATC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-MATC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-MATL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MATL-utf8.csv -> /tmp/ESC-MATL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-MATL-utf8.csv\n",
            "Subiendo: /tmp/ESC-MATL-utf8.csv -> gs://vl_00csv_01/ESC-MATL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-MATL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-MATL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-MESE-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MESE-utf8.csv -> /tmp/ESC-MESE-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-MESE-utf8.csv\n",
            "Subiendo: /tmp/ESC-MESE-utf8.csv -> gs://vl_00csv_01/ESC-MESE-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-MESE-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-MESE-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-MODU-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MODU-utf8.csv -> /tmp/ESC-MODU-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-MODU-utf8.csv\n",
            "Subiendo: /tmp/ESC-MODU-utf8.csv -> gs://vl_00csv_01/ESC-MODU-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-MODU-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-MODU-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-MOTI-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MOTI-utf8.csv -> /tmp/ESC-MOTI-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-MOTI-utf8.csv\n",
            "Subiendo: /tmp/ESC-MOTI-utf8.csv -> gs://vl_00csv_01/ESC-MOTI-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-MOTI-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-MOTI-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-PAGC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGC-utf8.csv -> /tmp/ESC-PAGC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-PAGC-utf8.csv\n",
            "Subiendo: /tmp/ESC-PAGC-utf8.csv -> gs://vl_00csv_01/ESC-PAGC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-PAGC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-PAGC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-PAGD-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGD-utf8.csv -> /tmp/ESC-PAGD-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-PAGD-utf8.csv\n",
            "Subiendo: /tmp/ESC-PAGD-utf8.csv -> gs://vl_00csv_01/ESC-PAGD-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-PAGD-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-PAGD-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-PAGL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGL-utf8.csv -> /tmp/ESC-PAGL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-PAGL-utf8.csv\n",
            "Subiendo: /tmp/ESC-PAGL-utf8.csv -> gs://vl_00csv_01/ESC-PAGL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-PAGL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-PAGL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-REPA-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-REPA-utf8.csv -> /tmp/ESC-REPA-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-REPA-utf8.csv\n",
            "Subiendo: /tmp/ESC-REPA-utf8.csv -> gs://vl_00csv_01/ESC-REPA-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-REPA-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-REPA-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-SERI-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SERI-utf8.csv -> /tmp/ESC-SERI-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-SERI-utf8.csv\n",
            "Subiendo: /tmp/ESC-SERI-utf8.csv -> gs://vl_00csv_01/ESC-SERI-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-SERI-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-SERI-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-SERL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SERL-utf8.csv -> /tmp/ESC-SERL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-SERL-utf8.csv\n",
            "Subiendo: /tmp/ESC-SERL-utf8.csv -> gs://vl_00csv_01/ESC-SERL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-SERL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-SERL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-SESI-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SESI-utf8.csv -> /tmp/ESC-SESI-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-SESI-utf8.csv\n",
            "Subiendo: /tmp/ESC-SESI-utf8.csv -> gs://vl_00csv_01/ESC-SESI-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-SESI-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-SESI-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TALL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TALL-utf8.csv -> /tmp/ESC-TALL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TALL-utf8.csv\n",
            "Subiendo: /tmp/ESC-TALL-utf8.csv -> gs://vl_00csv_01/ESC-TALL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TALL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TALL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TARC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARC-utf8.csv -> /tmp/ESC-TARC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TARC-utf8.csv\n",
            "Subiendo: /tmp/ESC-TARC-utf8.csv -> gs://vl_00csv_01/ESC-TARC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TARC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TARC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TARE-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARE-utf8.csv -> /tmp/ESC-TARE-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TARE-utf8.csv\n",
            "Subiendo: /tmp/ESC-TARE-utf8.csv -> gs://vl_00csv_01/ESC-TARE-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TARE-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TARE-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TARL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARL-utf8.csv -> /tmp/ESC-TARL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TARL-utf8.csv\n",
            "Subiendo: /tmp/ESC-TARL-utf8.csv -> gs://vl_00csv_01/ESC-TARL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TARL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TARL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TITL-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITL-utf8.csv -> /tmp/ESC-TITL-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TITL-utf8.csv\n",
            "Subiendo: /tmp/ESC-TITL-utf8.csv -> gs://vl_00csv_01/ESC-TITL-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TITL-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TITL-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TITN-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITN-utf8.csv -> /tmp/ESC-TITN-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TITN-utf8.csv\n",
            "Subiendo: /tmp/ESC-TITN-utf8.csv -> gs://vl_00csv_01/ESC-TITN-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TITN-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TITN-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TITU-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITU-utf8.csv -> /tmp/ESC-TITU-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TITU-utf8.csv\n",
            "Subiendo: /tmp/ESC-TITU-utf8.csv -> gs://vl_00csv_01/ESC-TITU-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TITU-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TITU-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/ESC-TURN-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TURN-utf8.csv -> /tmp/ESC-TURN-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/ESC-TURN-utf8.csv\n",
            "Subiendo: /tmp/ESC-TURN-utf8.csv -> gs://vl_00csv_01/ESC-TURN-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/ESC-TURN-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/ESC-TURN-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-DIRECCIONES-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-DIRECCIONES-utf8.csv -> /tmp/MA-DIRECCIONES-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-DIRECCIONES-utf8.csv\n",
            "Subiendo: /tmp/MA-DIRECCIONES-utf8.csv -> gs://vl_00csv_01/MA-DIRECCIONES-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-DIRECCIONES-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-DIRECCIONES-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-ENINC-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-ENINC-utf8.csv -> /tmp/MA-ENINC-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-ENINC-utf8.csv\n",
            "Subiendo: /tmp/MA-ENINC-utf8.csv -> gs://vl_00csv_01/MA-ENINC-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-ENINC-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-ENINC-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-ENTIDADES-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-ENTIDADES-utf8.csv -> /tmp/MA-ENTIDADES-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-ENTIDADES-utf8.csv\n",
            "Subiendo: /tmp/MA-ENTIDADES-utf8.csv -> gs://vl_00csv_01/MA-ENTIDADES-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-ENTIDADES-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-ENTIDADES-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-FPAGO-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-FPAGO-utf8.csv -> /tmp/MA-FPAGO-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-FPAGO-utf8.csv\n",
            "Subiendo: /tmp/MA-FPAGO-utf8.csv -> gs://vl_00csv_01/MA-FPAGO-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-FPAGO-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-FPAGO-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-PAISES-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-PAISES-utf8.csv -> /tmp/MA-PAISES-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-PAISES-utf8.csv\n",
            "Subiendo: /tmp/MA-PAISES-utf8.csv -> gs://vl_00csv_01/MA-PAISES-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-PAISES-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-PAISES-utf8.csv\n",
            "\n",
            "Iniciando transferencia de 'velneo/MA-PROVINCIAS-utf8.csv'\n",
            "Descargando: s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-PROVINCIAS-utf8.csv -> /tmp/MA-PROVINCIAS-utf8.csv\n",
            "Archivo descargado correctamente: /tmp/MA-PROVINCIAS-utf8.csv\n",
            "Subiendo: /tmp/MA-PROVINCIAS-utf8.csv -> gs://vl_00csv_01/MA-PROVINCIAS-utf8.csv\n",
            "Archivo subido correctamente: gs://vl_00csv_01/MA-PROVINCIAS-utf8.csv\n",
            "Eliminado archivo temporal: /tmp/MA-PROVINCIAS-utf8.csv\n",
            "\n",
            "=== Proceso de transferencia finalizado | 2025-02-10 08:32:44.264966 ===\n",
            "Duraci√≥n total: 0:00:25.595900\n",
            "\n",
            "=== Informe de transferencia ===\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-ANOS-utf8.csv -> gs://vl_00csv_01/ESC-ANOS-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-APET-utf8.csv -> gs://vl_00csv_01/ESC-APET-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-AULA-utf8.csv -> gs://vl_00csv_01/ESC-AULA-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-C&M-utf8.csv -> gs://vl_00csv_01/ESC-C&M-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-C&P-utf8.csv -> gs://vl_00csv_01/ESC-C&P-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CALE-utf8.csv -> gs://vl_00csv_01/ESC-CALE-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-COBR-utf8.csv -> gs://vl_00csv_01/ESC-COBR-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CONT-utf8.csv -> gs://vl_00csv_01/ESC-CONT-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CONV-utf8.csv -> gs://vl_00csv_01/ESC-CONV-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-CURS-utf8.csv -> gs://vl_00csv_01/ESC-CURS-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-DIAS-utf8.csv -> gs://vl_00csv_01/ESC-DIAS-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-DISC-utf8.csv -> gs://vl_00csv_01/ESC-DISC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVAC-utf8.csv -> gs://vl_00csv_01/ESC-EVAC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVAL-utf8.csv -> gs://vl_00csv_01/ESC-EVAL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EVEN-utf8.csv -> gs://vl_00csv_01/ESC-EVEN-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EXPE-utf8.csv -> gs://vl_00csv_01/ESC-EXPE-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-EXPN-utf8.csv -> gs://vl_00csv_01/ESC-EXPN-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-FACL-utf8.csv -> gs://vl_00csv_01/ESC-FACL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-FACT-utf8.csv -> gs://vl_00csv_01/ESC-FACT-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCC-utf8.csv -> gs://vl_00csv_01/ESC-GRCC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCL-utf8.csv -> gs://vl_00csv_01/ESC-GRCL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRCO-utf8.csv -> gs://vl_00csv_01/ESC-GRCO-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-GRUP-utf8.csv -> gs://vl_00csv_01/ESC-GRUP-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-L&T-utf8.csv -> gs://vl_00csv_01/ESC-L&T-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-LECC-utf8.csv -> gs://vl_00csv_01/ESC-LECC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MATC-utf8.csv -> gs://vl_00csv_01/ESC-MATC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MATL-utf8.csv -> gs://vl_00csv_01/ESC-MATL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MESE-utf8.csv -> gs://vl_00csv_01/ESC-MESE-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MODU-utf8.csv -> gs://vl_00csv_01/ESC-MODU-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-MOTI-utf8.csv -> gs://vl_00csv_01/ESC-MOTI-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGC-utf8.csv -> gs://vl_00csv_01/ESC-PAGC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGD-utf8.csv -> gs://vl_00csv_01/ESC-PAGD-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-PAGL-utf8.csv -> gs://vl_00csv_01/ESC-PAGL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-REPA-utf8.csv -> gs://vl_00csv_01/ESC-REPA-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SERI-utf8.csv -> gs://vl_00csv_01/ESC-SERI-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SERL-utf8.csv -> gs://vl_00csv_01/ESC-SERL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-SESI-utf8.csv -> gs://vl_00csv_01/ESC-SESI-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TALL-utf8.csv -> gs://vl_00csv_01/ESC-TALL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARC-utf8.csv -> gs://vl_00csv_01/ESC-TARC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARE-utf8.csv -> gs://vl_00csv_01/ESC-TARE-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TARL-utf8.csv -> gs://vl_00csv_01/ESC-TARL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITL-utf8.csv -> gs://vl_00csv_01/ESC-TITL-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITN-utf8.csv -> gs://vl_00csv_01/ESC-TITN-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TITU-utf8.csv -> gs://vl_00csv_01/ESC-TITU-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/ESC-TURN-utf8.csv -> gs://vl_00csv_01/ESC-TURN-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-DIRECCIONES-utf8.csv -> gs://vl_00csv_01/MA-DIRECCIONES-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-ENINC-utf8.csv -> gs://vl_00csv_01/MA-ENINC-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-ENTIDADES-utf8.csv -> gs://vl_00csv_01/MA-ENTIDADES-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-FPAGO-utf8.csv -> gs://vl_00csv_01/MA-FPAGO-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-PAISES-utf8.csv -> gs://vl_00csv_01/MA-PAISES-utf8.csv | Estado: Transferido con √©xito\n",
            "- s3://animum-datalake-landing-euwest3-637423635409/velneo/MA-PROVINCIAS-utf8.csv -> gs://vl_00csv_01/MA-PROVINCIAS-utf8.csv | Estado: Transferido con √©xito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CARGA DE GCS CSV A GBQ\n",
        "\n",
        "params = {\n",
        "    \"project_id\": \"animum-dev-datawarehouse\",\n",
        "    \"source_GCS_bucket_name\": \"vl_00csv_01\",\n",
        "    \"source_GCS_file_names_list\": [],  # Si se deja vac√≠o, se tomar√°n todos los CSV filtrados\n",
        "    \"source_GCS_file_names_filter\": {\n",
        "        \"use_bool\": True,\n",
        "        \"name_include_patterns_list\": [\"utf8\"],\n",
        "        \"name_exclude_patterns_list\": [],\n",
        "        \"extension_include_patterns_list\": [\".csv\"],\n",
        "        \"extension_exclude_patterns_list\": [],\n",
        "        \"min_size_kb\": 10,\n",
        "        \"max_size_kb\": 5000000,\n",
        "        \"modified_after_date\": \"2023-01-01\",\n",
        "        \"modified_before_date\": \"2050-12-31\",\n",
        "        \"include_subfolders_bool\": False\n",
        "    },\n",
        "    \"destination_GBQ_dataset_id\": \"vl_01raw_01\",\n",
        "    \"reemplazos\": {\n",
        "        \"-utf8\": \"\"\n",
        "    },\n",
        "    \"inference_limit\": 1000,       # N√∫mero de filas muestreadas para inferencia\n",
        "    \"inference_threshold\": 0.95    # Umbral m√≠nimo (0.95 = 95%) para asignar tipo de dato\n",
        "}\n",
        "\n",
        "# Ejemplo de llamada:\n",
        "if __name__ == \"__main__\":\n",
        "    GCS_load_CSV_to_GBQ(params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjhTxGj714qR",
        "outputId": "8f5518ad-c555-4e8e-8bdf-39fe4a28ab58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Procesando archivo: ESC-APET-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-APET-utf8.csv' cargado en DataFrame (columnas: 24)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_APET__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_APET__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_APET' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_APET' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_APET__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-C&M-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-C&M-utf8.csv' cargado en DataFrame (columnas: 11)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_M__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_C_M__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_M' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_M' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_M__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-C&P-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-C&P-utf8.csv' cargado en DataFrame (columnas: 12)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_P__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_C_P__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_P' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_P' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_C_P__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-COBR-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-COBR-utf8.csv' cargado en DataFrame (columnas: 34)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_COBR__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_COBR__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_COBR' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_COBR' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_COBR__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-CONT-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (35,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-CONT-utf8.csv' cargado en DataFrame (columnas: 41)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONT__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_CONT__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONT' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONT' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONT__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-CONV-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-CONV-utf8.csv' cargado en DataFrame (columnas: 68)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONV__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_CONV__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONV' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONV' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CONV__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-CURS-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-CURS-utf8.csv' cargado en DataFrame (columnas: 28)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CURS__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_CURS__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CURS' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_CURS' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_CURS__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-DIAS-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-DIAS-utf8.csv' cargado en DataFrame (columnas: 18)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_DIAS__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_DIAS__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_DIAS' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_DIAS' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_DIAS__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-EVAC-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-EVAC-utf8.csv' cargado en DataFrame (columnas: 24)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_EVAC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-EVAL-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (24,26) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-EVAL-utf8.csv' cargado en DataFrame (columnas: 30)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_EVAL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVAL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-EVEN-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (35,37,38,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-EVEN-utf8.csv' cargado en DataFrame (columnas: 41)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVEN__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_EVEN__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVEN' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVEN' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EVEN__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-EXPE-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-EXPE-utf8.csv' cargado en DataFrame (columnas: 28)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPE__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_EXPE__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPE' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPE' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPE__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-EXPN-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-EXPN-utf8.csv' cargado en DataFrame (columnas: 14)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPN__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_EXPN__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPN' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPN' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_EXPN__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-FACL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-FACL-utf8.csv' cargado en DataFrame (columnas: 13)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_FACL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-FACT-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (26,28,29,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-FACT-utf8.csv' cargado en DataFrame (columnas: 32)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACT__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_FACT__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACT' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACT' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_FACT__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-GRCC-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-GRCC-utf8.csv' cargado en DataFrame (columnas: 25)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_GRCC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-GRCL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-GRCL-utf8.csv' cargado en DataFrame (columnas: 16)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_GRCL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRCL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-GRUP-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-GRUP-utf8.csv' cargado en DataFrame (columnas: 18)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRUP__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_GRUP__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRUP' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRUP' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_GRUP__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-L&T-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-L&T-utf8.csv' cargado en DataFrame (columnas: 14)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_L_T__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_L_T__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_L_T' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_L_T' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_L_T__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-LECC-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-LECC-utf8.csv' cargado en DataFrame (columnas: 31)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_LECC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_LECC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_LECC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_LECC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_LECC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-MATC-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (11,15,17,27,28,32,33,34,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-MATC-utf8.csv' cargado en DataFrame (columnas: 77)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_MATC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-MATL-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'ESC-MATL-utf8.csv' cargado en DataFrame (columnas: 50)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_MATL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MATL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-MESE-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-MESE-utf8.csv' cargado en DataFrame (columnas: 10)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MESE__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_MESE__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MESE' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MESE' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MESE__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-MODU-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-MODU-utf8.csv' cargado en DataFrame (columnas: 42)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MODU__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_MODU__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MODU' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_MODU' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_MODU__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-PAGC-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-PAGC-utf8.csv' cargado en DataFrame (columnas: 24)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_PAGC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-PAGD-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-PAGD-utf8.csv' cargado en DataFrame (columnas: 19)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGD__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_PAGD__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGD' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGD' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGD__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-PAGL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-PAGL-utf8.csv' cargado en DataFrame (columnas: 7)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_PAGL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_PAGL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-SESI-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-SESI-utf8.csv' cargado en DataFrame (columnas: 27)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_SESI__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_SESI__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_SESI' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_SESI' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_SESI__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TALL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TALL-utf8.csv' cargado en DataFrame (columnas: 12)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TALL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TALL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TALL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TALL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TALL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TARC-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TARC-utf8.csv' cargado en DataFrame (columnas: 4)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TARC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TARE-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TARE-utf8.csv' cargado en DataFrame (columnas: 27)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARE__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TARE__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARE' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARE' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARE__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TARL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TARL-utf8.csv' cargado en DataFrame (columnas: 8)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TARL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TARL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TITL-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TITL-utf8.csv' cargado en DataFrame (columnas: 13)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITL__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TITL__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITL' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITL' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITL__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TITN-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TITN-utf8.csv' cargado en DataFrame (columnas: 11)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITN__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TITN__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITN' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITN' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITN__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TITU-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TITU-utf8.csv' cargado en DataFrame (columnas: 19)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITU__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TITU__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITU' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITU' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TITU__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: ESC-TURN-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'ESC-TURN-utf8.csv' cargado en DataFrame (columnas: 22)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TURN__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.ESC_TURN__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TURN' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.ESC_TURN' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.ESC_TURN__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: MA-DIRECCIONES-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (23,24,25,32,33,35,38,39,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'MA-DIRECCIONES-utf8.csv' cargado en DataFrame (columnas: 45)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_DIRECCIONES__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.MA_DIRECCIONES__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_DIRECCIONES' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_DIRECCIONES' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_DIRECCIONES__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: MA-ENINC-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'MA-ENINC-utf8.csv' cargado en DataFrame (columnas: 7)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_ENINC__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.MA_ENINC__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_ENINC' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_ENINC' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_ENINC__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: MA-ENTIDADES-utf8.csv ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-605044fd9e7a>:118: DtypeWarning: Columns (6,7,33,35,53,65,66,69,72,73,77,79,85,102,117,118,120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(io.StringIO(raw_text), delimiter=\";\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úîÔ∏è Archivo 'MA-ENTIDADES-utf8.csv' cargado en DataFrame (columnas: 124)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_ENTIDADES__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.MA_ENTIDADES__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_ENTIDADES' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_ENTIDADES' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_ENTIDADES__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: MA-PAISES-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'MA-PAISES-utf8.csv' cargado en DataFrame (columnas: 14)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_PAISES__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.MA_PAISES__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_PAISES' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_PAISES' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_PAISES__TMP' eliminada.\n",
            "\n",
            "=== Procesando archivo: MA-PROVINCIAS-utf8.csv ===\n",
            "‚úîÔ∏è Archivo 'MA-PROVINCIAS-utf8.csv' cargado en DataFrame (columnas: 10)\n",
            "üì§ Cargando datos a la tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_PROVINCIAS__TMP'...\n",
            "‚úîÔ∏è Carga finalizada en la tabla temporal: animum-dev-datawarehouse.vl_01raw_01.MA_PROVINCIAS__TMP\n",
            "üîé Inferencia de tipos en la tabla temporal (muestreo)...\n",
            "üì§ Creando/Reemplazando la tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_PROVINCIAS' con tipos inferidos...\n",
            "‚úîÔ∏è Tabla final 'animum-dev-datawarehouse.vl_01raw_01.MA_PROVINCIAS' creada con exito.\n",
            "üóëÔ∏è Tabla temporal 'animum-dev-datawarehouse.vl_01raw_01.MA_PROVINCIAS__TMP' eliminada.\n",
            "\n",
            "=== Proceso completo finalizado ===\n"
          ]
        }
      ]
    }
  ]
}